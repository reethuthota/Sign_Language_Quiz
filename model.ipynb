{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCek6yez-PRz"
      },
      "source": [
        "# Sign Language Quiz\n",
        "\n",
        "### Authors:  \n",
        "- <span style=\"color:lightblue;\">Avanthi Narasingu - 1RVU22CSE032</span>  \n",
        "- <span style=\"color:lightblue;\">Reethu RG Thota - 1RVU22CSE128</span>  \n",
        "- <span style=\"color:lightblue;\">Shreyas Rajiv - 1RVU22CSE153</span>\n",
        "\n",
        "### Guide:\n",
        "<span style=\"color:lightblue;\"> **Dr.Shobana Padmanabhan**</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3NIYcQc-TTS",
        "outputId": "25a6dd96-0f31-42d5-fc51-eb28e3a8e5c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqsTgaZwg-lq",
        "outputId": "9a29cc7f-816c-4913-db42-35bacb2b8d62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num GPUs Available: 1\n",
            "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Check if GPU is available\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    print(f\"Num GPUs Available: {len(gpus)}\")\n",
        "    for gpu in gpus:\n",
        "        print(gpu)\n",
        "else:\n",
        "    print(\"No GPU found.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TN0ehzcKg-lr",
        "outputId": "d12cc568-4a15-4fd0-a278-0363f7ad237c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing videos: 0it [00:00, ?it/s]I0000 00:00:1732715987.294684  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732715987.304041  613227 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732715987.309041  613230 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "/Users/reethu/Applications/anaconda3/lib/python3.11/site-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
            "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
            "Processing videos: 15it [00:00, 15.10it/s]I0000 00:00:1732715988.294164  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732715988.306554  613248 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732715988.316566  613248 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 30it [00:01, 17.11it/s]I0000 00:00:1732715989.080751  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732715989.086062  613284 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732715989.091071  613284 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 44it [00:02, 17.94it/s]I0000 00:00:1732715989.814440  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732715989.819473  613303 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732715989.824355  613304 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 83it [00:03, 30.99it/s]I0000 00:00:1732715990.553826  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732715990.559590  613325 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732715990.568621  613325 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 104it [00:03, 30.93it/s]I0000 00:00:1732715991.238334  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732715991.243592  613346 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732715991.249450  613346 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 108it [00:04, 21.78it/s]I0000 00:00:1732715992.029492  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732715992.034787  613364 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732715992.038965  613364 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 143it [00:05, 29.29it/s]I0000 00:00:1732715992.819521  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732715992.825692  613382 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732715992.830746  613382 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 156it [00:06, 25.03it/s]I0000 00:00:1732715993.615013  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732715993.620428  613400 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732715993.625357  613400 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 173it [00:06, 25.22it/s]I0000 00:00:1732715994.285691  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732715994.291772  613416 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732715994.296152  613416 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 186it [00:07, 25.05it/s]I0000 00:00:1732715994.807268  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732715994.813326  613439 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732715994.819134  613439 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 192it [00:08, 20.36it/s]I0000 00:00:1732715995.461781  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732715995.467033  613456 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732715995.471482  613456 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 200it [00:08, 17.09it/s]Context leak detected, msgtracer returned -1\n",
            "I0000 00:00:1732715996.222325  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732715996.229174  613473 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732715996.234288  613473 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 205it [00:09, 14.44it/s]I0000 00:00:1732715996.862559  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732715996.868418  613494 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732715996.872555  613494 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 210it [00:10, 12.51it/s]I0000 00:00:1732715997.499237  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732715997.503929  613516 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732715997.507874  613514 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 239it [00:10, 21.44it/s]I0000 00:00:1732715998.212714  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732715998.218848  613528 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732715998.223826  613528 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 269it [00:11, 29.55it/s]I0000 00:00:1732715998.787987  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732715998.793069  613554 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732715998.804006  613554 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 276it [00:12, 22.59it/s]I0000 00:00:1732715999.570785  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732715999.577451  613571 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732715999.586802  613571 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 279it [00:13, 16.16it/s]I0000 00:00:1732716000.395888  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716000.401205  613589 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716000.406530  613589 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 288it [00:13, 15.06it/s]I0000 00:00:1732716001.114432  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716001.119674  613606 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716001.124039  613606 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 292it [00:14, 12.16it/s]I0000 00:00:1732716001.855554  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716001.874637  613630 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716001.881981  613630 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 295it [00:15,  9.47it/s]I0000 00:00:1732716002.646339  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716002.652638  613647 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716002.659285  613647 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 303it [00:16,  9.98it/s]I0000 00:00:1732716003.356557  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716003.361834  613679 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716003.366985  613679 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 305it [00:16,  7.73it/s]I0000 00:00:1732716004.120875  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716004.126936  613697 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716004.132977  613697 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 314it [00:17,  8.64it/s]I0000 00:00:1732716004.979874  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716004.985041  613724 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716004.994274  613724 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 315it [00:18,  6.50it/s]I0000 00:00:1732716005.731562  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716005.738182  613745 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716005.743782  613745 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 381it [00:19, 29.41it/s]I0000 00:00:1732716006.567147  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716006.572307  613763 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716006.577089  613763 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 401it [00:19, 29.30it/s]I0000 00:00:1732716007.258164  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716007.263062  613784 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716007.266928  613788 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 413it [00:20, 26.32it/s]Context leak detected, msgtracer returned -1\n",
            "I0000 00:00:1732716007.916335  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716007.921975  613800 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716007.928170  613799 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 427it [00:21, 23.36it/s]I0000 00:00:1732716008.730473  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716008.735702  613826 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716008.741925  613826 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 440it [00:22, 21.72it/s]I0000 00:00:1732716009.463373  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716009.468756  613842 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716009.478296  613842 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 458it [00:22, 23.24it/s]I0000 00:00:1732716010.128683  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716010.135612  613874 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716010.145381  613874 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 473it [00:23, 24.65it/s]I0000 00:00:1732716010.639761  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716010.645566  613893 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716010.649938  613893 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 488it [00:24, 22.04it/s]I0000 00:00:1732716011.504120  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716011.509658  613918 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716011.516200  613918 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 530it [00:24, 32.96it/s]I0000 00:00:1732716012.214530  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716012.220965  613932 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716012.228422  613932 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 536it [00:25, 25.91it/s]I0000 00:00:1732716012.907510  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716012.913726  613953 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716012.919056  613953 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 539it [00:26, 18.30it/s]I0000 00:00:1732716013.755050  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716013.760890  613980 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716013.765155  613983 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 544it [00:27, 14.84it/s]I0000 00:00:1732716014.503118  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716014.508668  614000 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716014.513211  614000 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 577it [00:27, 24.47it/s]I0000 00:00:1732716015.166175  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716015.178425  614032 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716015.197679  614032 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 635it [00:28, 39.25it/s]I0000 00:00:1732716016.005877  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716016.011239  614047 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716016.018296  614049 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 697it [00:29, 52.44it/s]I0000 00:00:1732716016.739884  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716016.745268  614089 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716016.751284  614089 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 703it [00:30, 35.66it/s]I0000 00:00:1732716017.734173  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716017.739072  614114 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716017.743931  614114 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 718it [00:31, 32.28it/s]I0000 00:00:1732716018.400966  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716018.406769  614126 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716018.411491  614130 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 728it [00:31, 27.92it/s]I0000 00:00:1732716019.051182  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716019.057205  614148 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716019.068143  614150 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 762it [00:32, 33.20it/s]I0000 00:00:1732716019.795305  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716019.800557  614166 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716019.805891  614166 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 767it [00:33, 25.57it/s]Context leak detected, msgtracer returned -1\n",
            "I0000 00:00:1732716020.506379  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716020.512339  614183 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716020.518663  614183 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 778it [00:33, 22.54it/s]I0000 00:00:1732716021.229366  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716021.238280  614208 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716021.254847  614208 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 785it [00:34, 17.88it/s]I0000 00:00:1732716022.063889  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716022.070289  614223 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716022.075038  614223 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 808it [00:35, 22.96it/s]I0000 00:00:1732716022.672509  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716022.678558  614242 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716022.683184  614242 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 825it [00:36, 23.07it/s]I0000 00:00:1732716023.401307  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716023.406455  614263 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716023.410636  614263 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 860it [00:36, 30.13it/s]I0000 00:00:1732716024.171131  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716024.176526  614283 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716024.180461  614285 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 890it [00:37, 34.62it/s]I0000 00:00:1732716024.813783  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716024.819984  614305 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716024.824604  614305 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 914it [00:38, 32.02it/s]I0000 00:00:1732716025.697747  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716025.703011  614328 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716025.707181  614328 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 918it [00:39, 23.36it/s]I0000 00:00:1732716026.526063  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716026.532936  614352 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716026.536952  614352 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 939it [00:39, 24.99it/s]I0000 00:00:1732716027.241847  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716027.250067  614368 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716027.259492  614372 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 1, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716029.522290  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716029.528202  614390 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716029.533792  614390 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 2, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716030.874513  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716030.880707  614422 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716030.885868  614422 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 942it [00:44,  7.01it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 3, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716032.231270  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716032.237042  614449 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716032.241679  614449 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "I0000 00:00:1732716032.922390  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716032.928149  614477 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716032.936841  614477 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 944it [00:46,  5.58it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 4, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716033.812853  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716033.819095  614496 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716033.824660  614496 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 5, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716035.788058  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716035.794060  614526 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716035.800037  614532 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 946it [00:50,  3.14it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 6, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716037.893108  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716037.898593  614560 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716037.904478  614561 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 947it [00:52,  2.55it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 7, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Context leak detected, msgtracer returned -1\n",
            "I0000 00:00:1732716039.671964  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716039.739733  614592 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716039.747204  614592 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 948it [00:53,  2.29it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 8, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716040.708995  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716040.715207  614615 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716040.719589  614615 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 949it [00:54,  2.05it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 9, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716041.718787  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716041.723883  614636 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716041.728401  614636 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 950it [00:56,  1.59it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 10, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716043.453468  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716043.458488  614660 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716043.462986  614660 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 951it [00:57,  1.36it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 11, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716044.871308  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716044.877335  614680 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716044.883008  614680 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 952it [00:58,  1.29it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 12, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716045.858802  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716045.864688  614700 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716045.869352  614700 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 953it [00:59,  1.15it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 13, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716047.128390  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716047.134929  614724 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716047.147644  614724 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 954it [01:01,  1.06it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 14, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716048.338946  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716048.346566  614743 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716048.351300  614743 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 955it [01:02,  1.12s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 15, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716050.026538  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716050.031936  614764 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716050.037552  614763 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 956it [01:04,  1.29s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 16, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716051.863590  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716051.872850  614784 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716051.885868  614784 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 957it [01:05,  1.30s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 17, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716053.166395  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716053.173131  614802 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716053.184456  614806 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 958it [01:07,  1.40s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 18, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716054.849457  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716054.856908  614821 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716054.862189  614821 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 959it [01:10,  1.73s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 19, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716057.407157  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716057.413677  614853 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716057.422853  614856 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 960it [01:10,  1.45s/it]I0000 00:00:1732716058.165861  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716058.170932  614883 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716058.174765  614879 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 961it [01:12,  1.43s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 20, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716059.554201  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716059.559357  614917 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716059.564471  614917 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 962it [01:14,  1.55s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 21, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716061.373016  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716061.378575  615011 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716061.384251  615011 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 963it [01:15,  1.64s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 22, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Context leak detected, msgtracer returned -1\n",
            "I0000 00:00:1732716063.231021  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716063.237165  615052 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716063.244727  615052 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 964it [01:17,  1.48s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 23, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716064.339185  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716064.345401  615099 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716064.350308  615099 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 965it [01:18,  1.40s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 24, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716065.532053  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716065.537190  615130 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716065.542250  615130 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 966it [01:19,  1.30s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 25, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716066.623971  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716066.633050  615173 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716066.643610  615173 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 967it [01:20,  1.27s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 26, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716067.806516  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716067.812440  615190 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716067.818994  615190 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 968it [01:21,  1.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 27, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716069.046173  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716069.051147  615214 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716069.063647  615214 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 969it [01:22,  1.22s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 28, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716070.176167  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716070.181443  615232 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716070.185353  615232 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 970it [01:25,  1.53s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 29, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716072.429376  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716072.434895  615271 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716072.439170  615271 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 971it [01:26,  1.39s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 30, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716073.488074  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716073.494288  615290 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716073.499012  615296 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 972it [01:27,  1.34s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 31, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716074.718024  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716074.728499  615331 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716074.740382  615331 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 973it [01:29,  1.44s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 32, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716076.392997  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716076.398444  615399 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716076.403234  615399 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 974it [01:32,  2.15s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 33, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716080.195744  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716080.201637  615421 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716080.208136  615427 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 975it [01:34,  1.97s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 34, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716081.749579  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716081.756446  615445 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716081.775914  615445 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 976it [01:38,  2.52s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 35, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716085.533388  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716085.538956  615478 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716085.544129  615478 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 977it [01:39,  2.09s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 36, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716086.633959  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716086.641319  615510 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716086.665728  615512 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 978it [01:41,  2.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 37, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716088.531920  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716088.539015  615592 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716088.543811  615592 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 979it [01:43,  1.99s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 38, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716090.415600  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716090.422099  615615 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716090.434530  615615 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 980it [01:45,  2.08s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 39, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Context leak detected, msgtracer returned -1\n",
            "I0000 00:00:1732716092.697601  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716092.707821  615648 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716092.712862  615648 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 981it [01:47,  2.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 40, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716094.631532  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716094.636431  615691 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716094.640914  615691 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 982it [01:48,  1.68s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 41, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716095.482267  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716095.488287  615710 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716095.495192  615710 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 983it [01:52,  2.49s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 42, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716099.856962  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716099.863079  615766 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716099.871613  615766 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 984it [01:54,  2.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 43, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716101.338346  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716101.346517  615796 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716101.352455  615796 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 985it [01:56,  2.28s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 44, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716103.855272  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716103.863460  615848 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716103.875879  615848 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 986it [01:58,  2.32s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 45, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716106.263720  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716106.270063  615891 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716106.275849  615891 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 987it [02:00,  1.95s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 46, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716107.329444  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716107.337988  615922 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716107.349630  615922 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 988it [02:02,  2.08s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 47, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716109.722315  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716109.728970  615960 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716109.734275  615960 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 989it [02:04,  2.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 48, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716111.590717  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716111.596257  615979 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716111.608656  615979 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 990it [02:06,  2.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 49, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716113.597977  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716113.604821  616023 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716113.612754  616023 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 991it [02:08,  1.99s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 50, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716115.541358  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716115.547522  616063 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716115.554529  616063 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 992it [02:09,  1.68s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 51, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716116.499433  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716116.505866  616089 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716116.514793  616092 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 993it [02:10,  1.59s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 52, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716117.884274  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716117.890699  616137 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716117.896537  616137 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 994it [02:12,  1.54s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 53, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716119.294738  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716119.301125  616164 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716119.309632  616164 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 995it [02:13,  1.52s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 54, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716120.772397  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716120.778390  616194 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716120.792353  616194 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 996it [02:14,  1.47s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 55, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716122.133082  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716122.139443  616215 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716122.144577  616215 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 997it [02:15,  1.37s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 56, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Context leak detected, msgtracer returned -1\n",
            "I0000 00:00:1732716123.254672  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716123.261190  616240 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716123.265994  616240 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 998it [02:17,  1.30s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 57, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716124.410986  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716124.422039  616263 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716124.434690  616261 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 999it [02:18,  1.37s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 58, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716125.929869  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716125.936573  616282 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716125.943642  616282 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1000it [02:19,  1.32s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 59, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716127.125889  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716127.132356  616306 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716127.137234  616306 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1001it [02:21,  1.28s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 60, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716128.328770  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716128.335008  616321 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716128.340906  616322 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1002it [02:22,  1.42s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 61, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716130.060210  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716130.066999  616356 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716130.078250  616356 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1003it [02:24,  1.62s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 62, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716132.139357  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716132.145722  616382 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716132.150758  616382 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1004it [02:26,  1.59s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 63, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716133.696666  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716133.703991  616401 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716133.718312  616401 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1005it [02:27,  1.48s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 64, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716134.883928  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716134.889613  616422 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716134.894838  616422 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1006it [02:29,  1.66s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 65, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716136.965283  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716136.971296  616489 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716136.976402  616489 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1007it [02:31,  1.69s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 66, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716138.741930  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716138.748325  616516 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716138.754318  616516 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1008it [02:32,  1.58s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 67, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716140.062720  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716140.068503  616549 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716140.073144  616549 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1009it [02:34,  1.59s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 68, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716141.663061  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716141.669929  616589 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716141.677794  616589 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1010it [02:35,  1.48s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 69, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716142.881204  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716142.887121  616617 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716142.892370  616617 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1011it [02:37,  1.68s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 70, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716145.034187  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716145.040790  616666 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716145.048392  616666 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1012it [02:38,  1.46s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 71, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716145.967757  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716145.973491  616681 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716145.985455  616681 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1013it [02:40,  1.48s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 72, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Context leak detected, msgtracer returned -1\n",
            "I0000 00:00:1732716147.498499  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716147.503871  616721 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716147.509311  616721 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1014it [02:43,  2.09s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 73, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716151.015652  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716151.021628  616762 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716151.028220  616762 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1015it [02:46,  2.15s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 74, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716153.330399  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716153.337073  616787 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716153.354363  616787 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1016it [02:47,  1.87s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 75, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716154.513928  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716154.518994  616812 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716154.523853  616812 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1017it [02:49,  2.06s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 76, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716157.039905  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716157.045800  616839 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716157.050371  616839 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1018it [02:51,  2.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 77, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716159.255895  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716159.262698  616860 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716159.270431  616860 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1019it [02:52,  1.76s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 78, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716160.209053  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716160.214513  616897 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716160.219228  616896 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1020it [02:54,  1.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 79, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716161.330947  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716161.336125  616925 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716161.340994  616925 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1021it [02:56,  1.91s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 80, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716164.040668  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716164.046593  617035 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716164.052978  617035 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1022it [02:58,  1.75s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 81, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716165.408122  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716165.414344  617064 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716165.419581  617064 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1023it [02:59,  1.60s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 82, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716166.669895  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716166.675641  617083 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716166.683495  617083 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1024it [03:00,  1.55s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 83, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716168.090271  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716168.124498  617118 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716168.134364  617118 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1025it [03:02,  1.65s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 84, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716169.983221  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716169.989307  617142 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716169.997058  617143 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1026it [03:04,  1.73s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 85, Skipped: 885, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716171.908848  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716171.916034  617204 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716171.922877  617204 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1034it [03:05,  2.17it/s]I0000 00:00:1732716172.626741  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716172.633358  617227 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716172.638952  617227 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1035it [03:06,  1.99it/s]I0000 00:00:1732716173.421750  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716173.427795  617247 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716173.431946  617247 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1036it [03:06,  1.87it/s]I0000 00:00:1732716174.159763  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716174.165987  617266 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716174.172835  617266 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1045it [03:07,  4.21it/s]Context leak detected, msgtracer returned -1\n",
            "I0000 00:00:1732716174.886676  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716174.892206  617287 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716174.897195  617287 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1047it [03:08,  3.86it/s]I0000 00:00:1732716175.605700  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716175.611604  617300 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716175.616508  617300 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1049it [03:09,  3.14it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 86, Skipped: 902, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716176.721611  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716176.727049  617330 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716176.731563  617330 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1050it [03:10,  2.53it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 87, Skipped: 902, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716177.662016  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716177.667880  617352 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716177.673099  617352 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1051it [03:11,  2.15it/s]I0000 00:00:1732716178.519645  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716178.525492  617371 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716178.529988  617371 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1052it [03:13,  1.39it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 88, Skipped: 902, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716180.428533  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716180.434310  617392 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716180.438698  617392 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1053it [03:14,  1.24it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 89, Skipped: 902, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716181.560861  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716181.565503  617415 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716181.572982  617415 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1054it [03:15,  1.07it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 90, Skipped: 902, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716182.953036  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716182.958584  617432 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716182.962984  617432 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1055it [03:17,  1.04s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 91, Skipped: 902, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716184.306020  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716184.311805  617450 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716184.317332  617453 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1056it [03:19,  1.31s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 92, Skipped: 902, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716186.393924  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716186.399617  617475 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716186.404788  617475 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1057it [03:21,  1.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 93, Skipped: 902, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716188.683660  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716188.690868  617493 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716188.702963  617493 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1058it [03:23,  1.73s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 94, Skipped: 902, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716190.843144  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716190.849014  617517 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716190.853030  617517 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1070it [03:24,  2.60it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 95, Skipped: 913, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716192.062199  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716192.069531  617543 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716192.079740  617547 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1071it [03:26,  2.00it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 96, Skipped: 913, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716193.744311  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716193.750974  617563 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716193.762163  617567 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1072it [03:28,  1.46it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 97, Skipped: 913, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716195.847435  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716195.853378  617587 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716195.858856  617587 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1073it [03:29,  1.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 98, Skipped: 913, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716196.939521  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716196.945081  617624 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716196.949095  617624 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1074it [03:31,  1.03it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 99, Skipped: 913, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716199.086648  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716199.092327  617654 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716199.098702  617654 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1075it [03:33,  1.05s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 100, Skipped: 913, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Context leak detected, msgtracer returned -1\n",
            "I0000 00:00:1732716200.438694  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716200.444528  617671 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716200.450644  617671 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1076it [03:34,  1.08s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 101, Skipped: 913, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716201.628355  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716201.634485  617690 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716201.640309  617690 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1077it [03:35,  1.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 102, Skipped: 913, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716203.234367  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716203.240230  617723 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716203.244957  617723 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1078it [03:37,  1.21s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 103, Skipped: 913, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716204.485106  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716204.491416  617740 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716204.500704  617736 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1079it [03:38,  1.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 104, Skipped: 913, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716205.578453  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716205.584752  617757 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716205.590583  617757 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1080it [03:39,  1.21s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 105, Skipped: 913, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716206.876976  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716206.882206  617787 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716206.886144  617787 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1081it [03:40,  1.22s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 106, Skipped: 913, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716208.112249  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716208.117228  617810 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716208.121488  617810 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1082it [03:42,  1.47s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 107, Skipped: 913, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716210.218706  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716210.223806  617835 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716210.228287  617835 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1083it [03:44,  1.63s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 108, Skipped: 913, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716212.219133  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716212.224383  617862 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716212.230401  617862 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1084it [03:46,  1.56s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 109, Skipped: 913, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716213.602585  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716213.608049  617881 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716213.613179  617881 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1085it [03:47,  1.39s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 110, Skipped: 913, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716214.588263  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716214.593422  617904 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716214.598283  617904 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1086it [03:49,  1.56s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 111, Skipped: 913, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716216.568122  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716216.574955  617927 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716216.579802  617927 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1087it [03:50,  1.56s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 112, Skipped: 913, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716218.118108  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716218.123456  617943 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716218.128502  617943 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1088it [03:53,  2.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 113, Skipped: 913, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716221.218600  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716221.224574  617985 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716221.230929  617985 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1089it [03:55,  1.99s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 114, Skipped: 913, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716223.133168  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716223.139498  618008 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716223.144002  618008 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1090it [03:57,  1.81s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 115, Skipped: 913, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716224.509419  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716224.515684  618028 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716224.552051  618028 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1091it [03:58,  1.70s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 116, Skipped: 913, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Context leak detected, msgtracer returned -1\n",
            "I0000 00:00:1732716225.967655  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716225.974051  618044 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716225.987815  618044 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1092it [03:59,  1.47s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 117, Skipped: 913, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716226.901500  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716226.907386  618070 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716226.913028  618070 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1093it [04:01,  1.47s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 118, Skipped: 913, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716228.359054  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716228.366126  618087 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716228.371348  618087 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1094it [04:03,  1.63s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 119, Skipped: 913, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716230.377853  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716230.384158  618116 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716230.390075  618114 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1095it [04:05,  1.74s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 120, Skipped: 913, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716232.371381  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716232.377538  618139 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716232.382738  618139 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1096it [04:07,  1.87s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 121, Skipped: 913, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716234.555301  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716234.562541  618166 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716234.569002  618166 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1097it [04:10,  2.36s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 122, Skipped: 913, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716238.063268  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716238.069185  618198 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716238.074528  618198 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1098it [04:11,  1.96s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 123, Skipped: 913, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716239.061529  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716239.067121  618215 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716239.071831  618215 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1099it [04:13,  1.97s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 124, Skipped: 913, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716241.061516  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716241.069196  618246 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716241.074405  618246 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1100it [04:16,  2.09s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 125, Skipped: 913, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716243.435087  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716243.440139  618266 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716243.444029  618266 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1101it [04:17,  1.78s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 126, Skipped: 913, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716244.495692  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716244.501031  618288 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716244.507430  618292 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1102it [04:18,  1.58s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 127, Skipped: 913, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716245.585548  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716245.590665  618308 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716245.594374  618308 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1103it [04:19,  1.34s/it]I0000 00:00:1732716246.362992  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716246.368030  618326 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716246.373241  618326 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1104it [04:20,  1.28s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 128, Skipped: 913, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716247.507828  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716247.513154  618349 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716247.519400  618350 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1105it [04:22,  1.52s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 129, Skipped: 913, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716249.606304  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716249.613057  618374 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716249.624374  618372 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1106it [04:24,  1.64s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 130, Skipped: 913, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716251.511533  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716251.520918  618403 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716251.533644  618403 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1107it [04:26,  1.73s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 131, Skipped: 913, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1732716253.471316  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716253.476661  618427 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716253.481684  618427 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1108it [04:27,  1.50s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 132, Skipped: 913, Total: 1120"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Context leak detected, msgtracer returned -1\n",
            "I0000 00:00:1732716254.404213  609865 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
            "W0000 00:00:1732716254.409519  618447 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1732716254.414694  618447 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Processing videos: 1120it [04:28,  4.17it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 133, Skipped: 913, Total: 1120\n",
            "Processing complete!\n",
            "Total videos: 1120\n",
            "Already processed (skipped): 924\n",
            "Newly processed: 133\n",
            "Total processed: 1057\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "import os\n",
        "import numpy as np\n",
        "import csv\n",
        "import warnings\n",
        "import logging\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Suppress specific warnings from Mediapipe\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module='mediapipe')\n",
        "logging.getLogger('mediapipe').setLevel(logging.ERROR)\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
        "\n",
        "class HandTrackingDynamic:\n",
        "    def __init__(self, mode=False, maxHands=2, detectionCon=0.5, trackCon=0.5):\n",
        "        self.__mode__ = mode\n",
        "        self.__maxHands__ = maxHands\n",
        "        self.__detectionCon__ = detectionCon\n",
        "        self.__trackCon__ = trackCon\n",
        "        self.handsMp = mp.solutions.hands\n",
        "        self.hands = self.handsMp.Hands(\n",
        "            static_image_mode=mode,\n",
        "            max_num_hands=maxHands,\n",
        "            min_detection_confidence=detectionCon,\n",
        "            min_tracking_confidence=trackCon\n",
        "        )\n",
        "        self.mpDraw = mp.solutions.drawing_utils\n",
        "\n",
        "    def findFingers(self, frame, draw=True):\n",
        "        imgRGB = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        self.results = self.hands.process(imgRGB)\n",
        "        if self.results.multi_hand_landmarks and draw:\n",
        "            for handLms in self.results.multi_hand_landmarks:\n",
        "                self.mpDraw.draw_landmarks(frame, handLms, self.handsMp.HAND_CONNECTIONS)\n",
        "        return frame\n",
        "\n",
        "    def findPosition(self, frame):\n",
        "        h, w, _ = frame.shape\n",
        "        left_hand = np.zeros((21, 3))\n",
        "        right_hand = np.zeros((21, 3))\n",
        "\n",
        "        if self.results.multi_hand_landmarks:\n",
        "            for hand_landmarks, hand_handedness in zip(self.results.multi_hand_landmarks, self.results.multi_handedness):\n",
        "                is_right = hand_handedness.classification[0].label == \"Right\"\n",
        "                hand_data = np.array([[lm.x * w, lm.y * h, lm.z] for lm in hand_landmarks.landmark])\n",
        "\n",
        "                if is_right:\n",
        "                    right_hand = hand_data\n",
        "                else:\n",
        "                    left_hand = hand_data\n",
        "\n",
        "        return np.concatenate([left_hand.flatten(), right_hand.flatten()])\n",
        "\n",
        "def extract_hand_landmarks(video_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    detector = HandTrackingDynamic(maxHands=2)\n",
        "    landmarks_list = []\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        detector.findFingers(frame, draw=False)\n",
        "        landmarks = detector.findPosition(frame)\n",
        "\n",
        "        if landmarks is not None:\n",
        "            landmarks_list.append(landmarks)\n",
        "\n",
        "    cap.release()\n",
        "    return landmarks_list\n",
        "\n",
        "def create_sequences(landmarks, timesteps=30):\n",
        "    sequences = []\n",
        "    landmarks = np.array(landmarks)\n",
        "\n",
        "    if len(landmarks) >= timesteps:\n",
        "        for i in range(len(landmarks) - timesteps + 1):\n",
        "            seq = landmarks[i:i + timesteps]\n",
        "            sequences.append(seq)\n",
        "\n",
        "    return np.array(sequences)\n",
        "\n",
        "def process_videos_from_csv(csv_file, video_folder, timesteps=30):\n",
        "    processed_folder = '/Users/reethu/coding/Projects/Sign-Language-Quiz/WLASL/processed_landmarks_from_csv'\n",
        "    os.makedirs(processed_folder, exist_ok=True)\n",
        "\n",
        "    # Get list of already processed files\n",
        "    existing_files = set(os.listdir(processed_folder))\n",
        "\n",
        "    with open(csv_file, 'r') as file:\n",
        "        reader = csv.reader(file)\n",
        "        total_videos = sum(1 for row in reader) - 1  # Exclude header row\n",
        "        file.seek(0)  # Reset reader position to start after counting\n",
        "\n",
        "        processed_count = 0\n",
        "        skipped_count = 0\n",
        "\n",
        "        # Use tqdm for progress bar\n",
        "        next(reader)  # Skip header row\n",
        "        for row in tqdm(reader, desc=\"Processing videos\"):\n",
        "            gloss, video_id = row\n",
        "            expected_output_file = f\"{gloss.replace(' ', '_')}_{video_id}.npy\"\n",
        "\n",
        "            if expected_output_file in existing_files:\n",
        "                skipped_count += 1\n",
        "                continue\n",
        "\n",
        "            video_path = os.path.join(video_folder, f\"{video_id}.mp4\")\n",
        "\n",
        "            if os.path.exists(video_path):\n",
        "                landmarks = extract_hand_landmarks(video_path)\n",
        "\n",
        "                if landmarks:\n",
        "                    sequences = create_sequences(landmarks, timesteps)\n",
        "\n",
        "                    if len(sequences) > 0:\n",
        "                        processed_file_path = os.path.join(\n",
        "                            processed_folder,\n",
        "                            expected_output_file\n",
        "                        )\n",
        "                        np.save(processed_file_path, sequences)\n",
        "                        processed_count += 1\n",
        "\n",
        "                        print(f\"\\rProcessed: {processed_count}, Skipped: {skipped_count}, \"\n",
        "                              f\"Total: {total_videos}\", end=\"\")\n",
        "\n",
        "    print(f\"\\nProcessing complete!\")\n",
        "    print(f\"Total videos: {total_videos}\")\n",
        "    print(f\"Already processed (skipped): {skipped_count}\")\n",
        "    print(f\"Newly processed: {processed_count}\")\n",
        "    print(f\"Total processed: {skipped_count + processed_count}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    csv_file = \"/Users/reethu/coding/Projects/Sign-Language-Quiz/WLASL/sorted_100.csv\"\n",
        "    video_folder = \"/Users/reethu/coding/Projects/Sign-Language-Quiz/WLASL/WLASL_dataset/videos\"\n",
        "    process_videos_from_csv(csv_file, video_folder)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g23GCOzSg-lt",
        "outputId": "412ba3e9-9bd6-4471-a281-3700b9c2da40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(31, 30, 126)\n",
            "(16, 30, 126)\n",
            "(37, 30, 126)\n",
            "(16, 30, 126)\n",
            "(16, 30, 126)\n",
            "(13, 30, 126)\n",
            "(12, 30, 126)\n",
            "(79, 30, 126)\n",
            "(32, 30, 126)\n",
            "(4, 30, 126)\n",
            "(34, 30, 126)\n",
            "(59, 30, 126)\n",
            "(4, 30, 126)\n",
            "(31, 30, 126)\n",
            "(59, 30, 126)\n",
            "(72, 30, 126)\n",
            "(43, 30, 126)\n",
            "(78, 30, 126)\n",
            "(52, 30, 126)\n",
            "(14, 30, 126)\n",
            "(28, 30, 126)\n",
            "(14, 30, 126)\n",
            "(24, 30, 126)\n",
            "(16, 30, 126)\n",
            "(11, 30, 126)\n",
            "(73, 30, 126)\n",
            "(44, 30, 126)\n",
            "(17, 30, 126)\n",
            "(34, 30, 126)\n",
            "(57, 30, 126)\n",
            "(38, 30, 126)\n",
            "(7, 30, 126)\n",
            "(11, 30, 126)\n",
            "(18, 30, 126)\n",
            "(32, 30, 126)\n",
            "(64, 30, 126)\n",
            "(29, 30, 126)\n",
            "(126, 30, 126)\n",
            "(25, 30, 126)\n",
            "(82, 30, 126)\n",
            "(16, 30, 126)\n",
            "(16, 30, 126)\n",
            "(41, 30, 126)\n",
            "(50, 30, 126)\n",
            "(41, 30, 126)\n",
            "(42, 30, 126)\n",
            "(42, 30, 126)\n",
            "(65, 30, 126)\n",
            "(67, 30, 126)\n",
            "(77, 30, 126)\n",
            "(102, 30, 126)\n",
            "(77, 30, 126)\n",
            "(30, 30, 126)\n",
            "(14, 30, 126)\n",
            "(10, 30, 126)\n",
            "(43, 30, 126)\n",
            "(46, 30, 126)\n",
            "(48, 30, 126)\n",
            "(42, 30, 126)\n",
            "(29, 30, 126)\n",
            "(39, 30, 126)\n",
            "(30, 30, 126)\n",
            "(30, 30, 126)\n",
            "(20, 30, 126)\n",
            "(24, 30, 126)\n",
            "(30, 30, 126)\n",
            "(33, 30, 126)\n",
            "(1, 30, 126)\n",
            "(24, 30, 126)\n",
            "(34, 30, 126)\n",
            "(53, 30, 126)\n",
            "(59, 30, 126)\n",
            "(3, 30, 126)\n",
            "(1, 30, 126)\n",
            "(57, 30, 126)\n",
            "(46, 30, 126)\n",
            "(29, 30, 126)\n",
            "(30, 30, 126)\n",
            "(33, 30, 126)\n",
            "(35, 30, 126)\n",
            "(65, 30, 126)\n",
            "(12, 30, 126)\n",
            "(166, 30, 126)\n",
            "(54, 30, 126)\n",
            "(43, 30, 126)\n",
            "(38, 30, 126)\n",
            "(39, 30, 126)\n",
            "(58, 30, 126)\n",
            "(29, 30, 126)\n",
            "(55, 30, 126)\n",
            "(29, 30, 126)\n",
            "(31, 30, 126)\n",
            "(19, 30, 126)\n",
            "(55, 30, 126)\n",
            "(40, 30, 126)\n",
            "(58, 30, 126)\n",
            "(4, 30, 126)\n",
            "(74, 30, 126)\n",
            "(37, 30, 126)\n",
            "(24, 30, 126)\n",
            "(21, 30, 126)\n",
            "(31, 30, 126)\n",
            "(29, 30, 126)\n",
            "(7, 30, 126)\n",
            "(53, 30, 126)\n",
            "(1, 30, 126)\n",
            "(32, 30, 126)\n",
            "(69, 30, 126)\n",
            "(22, 30, 126)\n",
            "(61, 30, 126)\n",
            "(32, 30, 126)\n",
            "(41, 30, 126)\n",
            "(27, 30, 126)\n",
            "(36, 30, 126)\n",
            "(7, 30, 126)\n",
            "(55, 30, 126)\n",
            "(67, 30, 126)\n",
            "(10, 30, 126)\n",
            "(17, 30, 126)\n",
            "(14, 30, 126)\n",
            "(61, 30, 126)\n",
            "(81, 30, 126)\n",
            "(23, 30, 126)\n",
            "(12, 30, 126)\n",
            "(37, 30, 126)\n",
            "(79, 30, 126)\n",
            "(6, 30, 126)\n",
            "(76, 30, 126)\n",
            "(80, 30, 126)\n",
            "(5, 30, 126)\n",
            "(15, 30, 126)\n",
            "(2, 30, 126)\n",
            "(6, 30, 126)\n",
            "(120, 30, 126)\n",
            "(46, 30, 126)\n",
            "(26, 30, 126)\n",
            "(27, 30, 126)\n",
            "(75, 30, 126)\n",
            "(11, 30, 126)\n",
            "(71, 30, 126)\n",
            "(4, 30, 126)\n",
            "(1, 30, 126)\n",
            "(66, 30, 126)\n",
            "(48, 30, 126)\n",
            "(73, 30, 126)\n",
            "(9, 30, 126)\n",
            "(41, 30, 126)\n",
            "(15, 30, 126)\n",
            "(4, 30, 126)\n",
            "(13, 30, 126)\n",
            "(62, 30, 126)\n",
            "(67, 30, 126)\n",
            "(4, 30, 126)\n",
            "(20, 30, 126)\n",
            "(30, 30, 126)\n",
            "(12, 30, 126)\n",
            "(38, 30, 126)\n",
            "(18, 30, 126)\n",
            "(26, 30, 126)\n",
            "(33, 30, 126)\n",
            "(73, 30, 126)\n",
            "(114, 30, 126)\n",
            "(73, 30, 126)\n",
            "(43, 30, 126)\n",
            "(39, 30, 126)\n",
            "(27, 30, 126)\n",
            "(26, 30, 126)\n",
            "(18, 30, 126)\n",
            "(24, 30, 126)\n",
            "(18, 30, 126)\n",
            "(34, 30, 126)\n",
            "(61, 30, 126)\n",
            "(42, 30, 126)\n",
            "(25, 30, 126)\n",
            "(34, 30, 126)\n",
            "(14, 30, 126)\n",
            "(59, 30, 126)\n",
            "(18, 30, 126)\n",
            "(68, 30, 126)\n",
            "(62, 30, 126)\n",
            "(58, 30, 126)\n",
            "(23, 30, 126)\n",
            "(45, 30, 126)\n",
            "(8, 30, 126)\n",
            "(73, 30, 126)\n",
            "(73, 30, 126)\n",
            "(53, 30, 126)\n",
            "(6, 30, 126)\n",
            "(5, 30, 126)\n",
            "(10, 30, 126)\n",
            "(92, 30, 126)\n",
            "(20, 30, 126)\n",
            "(79, 30, 126)\n",
            "(31, 30, 126)\n",
            "(8, 30, 126)\n",
            "(47, 30, 126)\n",
            "(52, 30, 126)\n",
            "(58, 30, 126)\n",
            "(68, 30, 126)\n",
            "(2, 30, 126)\n",
            "(5, 30, 126)\n",
            "(126, 30, 126)\n",
            "(40, 30, 126)\n",
            "(31, 30, 126)\n",
            "(35, 30, 126)\n",
            "(43, 30, 126)\n",
            "(71, 30, 126)\n",
            "(49, 30, 126)\n",
            "(70, 30, 126)\n",
            "(66, 30, 126)\n",
            "(86, 30, 126)\n",
            "(10, 30, 126)\n",
            "(48, 30, 126)\n",
            "(45, 30, 126)\n",
            "(68, 30, 126)\n",
            "(73, 30, 126)\n",
            "(21, 30, 126)\n",
            "(43, 30, 126)\n",
            "(16, 30, 126)\n",
            "(40, 30, 126)\n",
            "(55, 30, 126)\n",
            "(59, 30, 126)\n",
            "(2, 30, 126)\n",
            "(46, 30, 126)\n",
            "(70, 30, 126)\n",
            "(47, 30, 126)\n",
            "(69, 30, 126)\n",
            "(22, 30, 126)\n",
            "(57, 30, 126)\n",
            "(51, 30, 126)\n",
            "(52, 30, 126)\n",
            "(61, 30, 126)\n",
            "(57, 30, 126)\n",
            "(4, 30, 126)\n",
            "(57, 30, 126)\n",
            "(34, 30, 126)\n",
            "(25, 30, 126)\n",
            "(28, 30, 126)\n",
            "(3, 30, 126)\n",
            "(12, 30, 126)\n",
            "(40, 30, 126)\n",
            "(16, 30, 126)\n",
            "(7, 30, 126)\n",
            "(49, 30, 126)\n",
            "(54, 30, 126)\n",
            "(6, 30, 126)\n",
            "(31, 30, 126)\n",
            "(33, 30, 126)\n",
            "(61, 30, 126)\n",
            "(21, 30, 126)\n",
            "(70, 30, 126)\n",
            "(11, 30, 126)\n",
            "(23, 30, 126)\n",
            "(68, 30, 126)\n",
            "(61, 30, 126)\n",
            "(28, 30, 126)\n",
            "(2, 30, 126)\n",
            "(4, 30, 126)\n",
            "(68, 30, 126)\n",
            "(18, 30, 126)\n",
            "(65, 30, 126)\n",
            "(62, 30, 126)\n",
            "(46, 30, 126)\n",
            "(61, 30, 126)\n",
            "(44, 30, 126)\n",
            "(74, 30, 126)\n",
            "(1, 30, 126)\n",
            "(11, 30, 126)\n",
            "(25, 30, 126)\n",
            "(61, 30, 126)\n",
            "(35, 30, 126)\n",
            "(46, 30, 126)\n",
            "(14, 30, 126)\n",
            "(4, 30, 126)\n",
            "(63, 30, 126)\n",
            "(68, 30, 126)\n",
            "(4, 30, 126)\n",
            "(2, 30, 126)\n",
            "(90, 30, 126)\n",
            "(65, 30, 126)\n",
            "(12, 30, 126)\n",
            "(24, 30, 126)\n",
            "(18, 30, 126)\n",
            "(43, 30, 126)\n",
            "(19, 30, 126)\n",
            "(51, 30, 126)\n",
            "(27, 30, 126)\n",
            "(5, 30, 126)\n",
            "(5, 30, 126)\n",
            "(21, 30, 126)\n",
            "(59, 30, 126)\n",
            "(6, 30, 126)\n",
            "(2, 30, 126)\n",
            "(72, 30, 126)\n",
            "(32, 30, 126)\n",
            "(45, 30, 126)\n",
            "(11, 30, 126)\n",
            "(1, 30, 126)\n",
            "(17, 30, 126)\n",
            "(66, 30, 126)\n",
            "(46, 30, 126)\n",
            "(43, 30, 126)\n",
            "(1, 30, 126)\n",
            "(9, 30, 126)\n",
            "(14, 30, 126)\n",
            "(60, 30, 126)\n",
            "(29, 30, 126)\n",
            "(39, 30, 126)\n",
            "(37, 30, 126)\n",
            "(2, 30, 126)\n",
            "(79, 30, 126)\n",
            "(53, 30, 126)\n",
            "(9, 30, 126)\n",
            "(9, 30, 126)\n",
            "(23, 30, 126)\n",
            "(33, 30, 126)\n",
            "(37, 30, 126)\n",
            "(14, 30, 126)\n",
            "(10, 30, 126)\n",
            "(38, 30, 126)\n",
            "(36, 30, 126)\n",
            "(54, 30, 126)\n",
            "(17, 30, 126)\n",
            "(78, 30, 126)\n",
            "(24, 30, 126)\n",
            "(15, 30, 126)\n",
            "(24, 30, 126)\n",
            "(39, 30, 126)\n",
            "(51, 30, 126)\n",
            "(24, 30, 126)\n",
            "(67, 30, 126)\n",
            "(58, 30, 126)\n",
            "(5, 30, 126)\n",
            "(52, 30, 126)\n",
            "(24, 30, 126)\n",
            "(37, 30, 126)\n",
            "(65, 30, 126)\n",
            "(41, 30, 126)\n",
            "(34, 30, 126)\n",
            "(12, 30, 126)\n",
            "(53, 30, 126)\n",
            "(65, 30, 126)\n",
            "(39, 30, 126)\n",
            "(53, 30, 126)\n",
            "(46, 30, 126)\n",
            "(45, 30, 126)\n",
            "(9, 30, 126)\n",
            "(16, 30, 126)\n",
            "(37, 30, 126)\n",
            "(32, 30, 126)\n",
            "(32, 30, 126)\n",
            "(40, 30, 126)\n",
            "(43, 30, 126)\n",
            "(41, 30, 126)\n",
            "(3, 30, 126)\n",
            "(65, 30, 126)\n",
            "(13, 30, 126)\n",
            "(20, 30, 126)\n",
            "(26, 30, 126)\n",
            "(92, 30, 126)\n",
            "(52, 30, 126)\n",
            "(58, 30, 126)\n",
            "(29, 30, 126)\n",
            "(48, 30, 126)\n",
            "(39, 30, 126)\n",
            "(67, 30, 126)\n",
            "(22, 30, 126)\n",
            "(17, 30, 126)\n",
            "(46, 30, 126)\n",
            "(28, 30, 126)\n",
            "(33, 30, 126)\n",
            "(56, 30, 126)\n",
            "(54, 30, 126)\n",
            "(49, 30, 126)\n",
            "(7, 30, 126)\n",
            "(59, 30, 126)\n",
            "(31, 30, 126)\n",
            "(12, 30, 126)\n",
            "(39, 30, 126)\n",
            "(72, 30, 126)\n",
            "(54, 30, 126)\n",
            "(52, 30, 126)\n",
            "(62, 30, 126)\n",
            "(15, 30, 126)\n",
            "(59, 30, 126)\n",
            "(41, 30, 126)\n",
            "(47, 30, 126)\n",
            "(33, 30, 126)\n",
            "(43, 30, 126)\n",
            "(75, 30, 126)\n",
            "(62, 30, 126)\n",
            "(37, 30, 126)\n",
            "(99, 30, 126)\n",
            "(44, 30, 126)\n",
            "(63, 30, 126)\n",
            "(59, 30, 126)\n",
            "(79, 30, 126)\n",
            "(15, 30, 126)\n",
            "(58, 30, 126)\n",
            "(49, 30, 126)\n",
            "(50, 30, 126)\n",
            "(43, 30, 126)\n",
            "(3, 30, 126)\n",
            "(24, 30, 126)\n",
            "(30, 30, 126)\n",
            "(4, 30, 126)\n",
            "(68, 30, 126)\n",
            "(64, 30, 126)\n",
            "(4, 30, 126)\n",
            "(5, 30, 126)\n",
            "(38, 30, 126)\n",
            "(2, 30, 126)\n",
            "(53, 30, 126)\n",
            "(53, 30, 126)\n",
            "(1, 30, 126)\n",
            "(29, 30, 126)\n",
            "(39, 30, 126)\n",
            "(40, 30, 126)\n",
            "(8, 30, 126)\n",
            "(40, 30, 126)\n",
            "(45, 30, 126)\n",
            "(32, 30, 126)\n",
            "(27, 30, 126)\n",
            "(42, 30, 126)\n",
            "(54, 30, 126)\n",
            "(57, 30, 126)\n",
            "(10, 30, 126)\n",
            "(67, 30, 126)\n",
            "(34, 30, 126)\n",
            "(15, 30, 126)\n",
            "(103, 30, 126)\n",
            "(67, 30, 126)\n",
            "(52, 30, 126)\n",
            "(55, 30, 126)\n",
            "(71, 30, 126)\n",
            "(48, 30, 126)\n",
            "(37, 30, 126)\n",
            "(75, 30, 126)\n",
            "(1, 30, 126)\n",
            "(42, 30, 126)\n",
            "(52, 30, 126)\n",
            "(58, 30, 126)\n",
            "(51, 30, 126)\n",
            "(32, 30, 126)\n",
            "(17, 30, 126)\n",
            "(19, 30, 126)\n",
            "(46, 30, 126)\n",
            "(62, 30, 126)\n",
            "(15, 30, 126)\n",
            "(57, 30, 126)\n",
            "(66, 30, 126)\n",
            "(56, 30, 126)\n",
            "(32, 30, 126)\n",
            "(42, 30, 126)\n",
            "(67, 30, 126)\n",
            "(53, 30, 126)\n",
            "(65, 30, 126)\n",
            "(10, 30, 126)\n",
            "(10, 30, 126)\n",
            "(2, 30, 126)\n",
            "(35, 30, 126)\n",
            "(6, 30, 126)\n",
            "(5, 30, 126)\n",
            "(69, 30, 126)\n",
            "(73, 30, 126)\n",
            "(20, 30, 126)\n",
            "(28, 30, 126)\n",
            "(45, 30, 126)\n",
            "(9, 30, 126)\n",
            "(6, 30, 126)\n",
            "(73, 30, 126)\n",
            "(11, 30, 126)\n",
            "(45, 30, 126)\n",
            "(22, 30, 126)\n",
            "(13, 30, 126)\n",
            "(15, 30, 126)\n",
            "(17, 30, 126)\n",
            "(76, 30, 126)\n",
            "(62, 30, 126)\n",
            "(4, 30, 126)\n",
            "(5, 30, 126)\n",
            "(7, 30, 126)\n",
            "(61, 30, 126)\n",
            "(22, 30, 126)\n",
            "(56, 30, 126)\n",
            "(58, 30, 126)\n",
            "(48, 30, 126)\n",
            "(44, 30, 126)\n",
            "(18, 30, 126)\n",
            "(24, 30, 126)\n",
            "(67, 30, 126)\n",
            "(6, 30, 126)\n",
            "(22, 30, 126)\n",
            "(26, 30, 126)\n",
            "(43, 30, 126)\n",
            "(92, 30, 126)\n",
            "(45, 30, 126)\n",
            "(1, 30, 126)\n",
            "(10, 30, 126)\n",
            "(6, 30, 126)\n",
            "(39, 30, 126)\n",
            "(53, 30, 126)\n",
            "(40, 30, 126)\n",
            "(49, 30, 126)\n",
            "(18, 30, 126)\n",
            "(56, 30, 126)\n",
            "(36, 30, 126)\n",
            "(15, 30, 126)\n",
            "(4, 30, 126)\n",
            "(43, 30, 126)\n",
            "(47, 30, 126)\n",
            "(22, 30, 126)\n",
            "(18, 30, 126)\n",
            "(51, 30, 126)\n",
            "(70, 30, 126)\n",
            "(39, 30, 126)\n",
            "(7, 30, 126)\n",
            "(39, 30, 126)\n",
            "(37, 30, 126)\n",
            "(53, 30, 126)\n",
            "(34, 30, 126)\n",
            "(3, 30, 126)\n",
            "(18, 30, 126)\n",
            "(3, 30, 126)\n",
            "(75, 30, 126)\n",
            "(40, 30, 126)\n",
            "(46, 30, 126)\n",
            "(4, 30, 126)\n",
            "(13, 30, 126)\n",
            "(15, 30, 126)\n",
            "(57, 30, 126)\n",
            "(15, 30, 126)\n",
            "(3, 30, 126)\n",
            "(36, 30, 126)\n",
            "(33, 30, 126)\n",
            "(74, 30, 126)\n",
            "(53, 30, 126)\n",
            "(47, 30, 126)\n",
            "(49, 30, 126)\n",
            "(5, 30, 126)\n",
            "(2, 30, 126)\n",
            "(6, 30, 126)\n",
            "(49, 30, 126)\n",
            "(31, 30, 126)\n",
            "(19, 30, 126)\n",
            "(55, 30, 126)\n",
            "(57, 30, 126)\n",
            "(8, 30, 126)\n",
            "(19, 30, 126)\n",
            "(7, 30, 126)\n",
            "(16, 30, 126)\n",
            "(10, 30, 126)\n",
            "(34, 30, 126)\n",
            "(15, 30, 126)\n",
            "(60, 30, 126)\n",
            "(19, 30, 126)\n",
            "(77, 30, 126)\n",
            "(19, 30, 126)\n",
            "(25, 30, 126)\n",
            "(3, 30, 126)\n",
            "(49, 30, 126)\n",
            "(28, 30, 126)\n",
            "(20, 30, 126)\n",
            "(94, 30, 126)\n",
            "(60, 30, 126)\n",
            "(36, 30, 126)\n",
            "(92, 30, 126)\n",
            "(68, 30, 126)\n",
            "(37, 30, 126)\n",
            "(67, 30, 126)\n",
            "(56, 30, 126)\n",
            "(8, 30, 126)\n",
            "(19, 30, 126)\n",
            "(19, 30, 126)\n",
            "(19, 30, 126)\n",
            "(74, 30, 126)\n",
            "(11, 30, 126)\n",
            "(76, 30, 126)\n",
            "(52, 30, 126)\n",
            "(22, 30, 126)\n",
            "(65, 30, 126)\n",
            "(50, 30, 126)\n",
            "(23, 30, 126)\n",
            "(24, 30, 126)\n",
            "(68, 30, 126)\n",
            "(29, 30, 126)\n",
            "(41, 30, 126)\n",
            "(29, 30, 126)\n",
            "(34, 30, 126)\n",
            "(62, 30, 126)\n",
            "(27, 30, 126)\n",
            "(51, 30, 126)\n",
            "(3, 30, 126)\n",
            "(15, 30, 126)\n",
            "(18, 30, 126)\n",
            "(48, 30, 126)\n",
            "(82, 30, 126)\n",
            "(28, 30, 126)\n",
            "(42, 30, 126)\n",
            "(40, 30, 126)\n",
            "(30, 30, 126)\n",
            "(6, 30, 126)\n",
            "(6, 30, 126)\n",
            "(49, 30, 126)\n",
            "(4, 30, 126)\n",
            "(15, 30, 126)\n",
            "(52, 30, 126)\n",
            "(30, 30, 126)\n",
            "(35, 30, 126)\n",
            "(73, 30, 126)\n",
            "(46, 30, 126)\n",
            "(70, 30, 126)\n",
            "(24, 30, 126)\n",
            "(43, 30, 126)\n",
            "(49, 30, 126)\n",
            "(7, 30, 126)\n",
            "(1, 30, 126)\n",
            "(60, 30, 126)\n",
            "(42, 30, 126)\n",
            "(38, 30, 126)\n",
            "(13, 30, 126)\n",
            "(6, 30, 126)\n",
            "(52, 30, 126)\n",
            "(17, 30, 126)\n",
            "(12, 30, 126)\n",
            "(57, 30, 126)\n",
            "(30, 30, 126)\n",
            "(55, 30, 126)\n",
            "(50, 30, 126)\n",
            "(9, 30, 126)\n",
            "(59, 30, 126)\n",
            "(59, 30, 126)\n",
            "(12, 30, 126)\n",
            "(35, 30, 126)\n",
            "(62, 30, 126)\n",
            "(62, 30, 126)\n",
            "(65, 30, 126)\n",
            "(56, 30, 126)\n",
            "(57, 30, 126)\n",
            "(32, 30, 126)\n",
            "(1, 30, 126)\n",
            "(4, 30, 126)\n",
            "(49, 30, 126)\n",
            "(21, 30, 126)\n",
            "(14, 30, 126)\n",
            "(47, 30, 126)\n",
            "(58, 30, 126)\n",
            "(28, 30, 126)\n",
            "(2, 30, 126)\n",
            "(3, 30, 126)\n",
            "(3, 30, 126)\n",
            "(35, 30, 126)\n",
            "(42, 30, 126)\n",
            "(20, 30, 126)\n",
            "(30, 30, 126)\n",
            "(63, 30, 126)\n",
            "(14, 30, 126)\n",
            "(19, 30, 126)\n",
            "(16, 30, 126)\n",
            "(87, 30, 126)\n",
            "(8, 30, 126)\n",
            "(31, 30, 126)\n",
            "(24, 30, 126)\n",
            "(42, 30, 126)\n",
            "(64, 30, 126)\n",
            "(12, 30, 126)\n",
            "(6, 30, 126)\n",
            "(73, 30, 126)\n",
            "(44, 30, 126)\n",
            "(9, 30, 126)\n",
            "(72, 30, 126)\n",
            "(21, 30, 126)\n",
            "(119, 30, 126)\n",
            "(56, 30, 126)\n",
            "(80, 30, 126)\n",
            "(50, 30, 126)\n",
            "(46, 30, 126)\n",
            "(8, 30, 126)\n",
            "(50, 30, 126)\n",
            "(8, 30, 126)\n",
            "(80, 30, 126)\n",
            "(43, 30, 126)\n",
            "(55, 30, 126)\n",
            "(43, 30, 126)\n",
            "(6, 30, 126)\n",
            "(55, 30, 126)\n",
            "(7, 30, 126)\n",
            "(82, 30, 126)\n",
            "(35, 30, 126)\n",
            "(37, 30, 126)\n",
            "(58, 30, 126)\n",
            "(53, 30, 126)\n",
            "(34, 30, 126)\n",
            "(62, 30, 126)\n",
            "(80, 30, 126)\n",
            "(76, 30, 126)\n",
            "(56, 30, 126)\n",
            "(18, 30, 126)\n",
            "(25, 30, 126)\n",
            "(19, 30, 126)\n",
            "(19, 30, 126)\n",
            "(45, 30, 126)\n",
            "(74, 30, 126)\n",
            "(59, 30, 126)\n",
            "(63, 30, 126)\n",
            "(41, 30, 126)\n",
            "(1, 30, 126)\n",
            "(6, 30, 126)\n",
            "(23, 30, 126)\n",
            "(63, 30, 126)\n",
            "(10, 30, 126)\n",
            "(42, 30, 126)\n",
            "(90, 30, 126)\n",
            "(66, 30, 126)\n",
            "(7, 30, 126)\n",
            "(7, 30, 126)\n",
            "(86, 30, 126)\n",
            "(45, 30, 126)\n",
            "(17, 30, 126)\n",
            "(6, 30, 126)\n",
            "(52, 30, 126)\n",
            "(43, 30, 126)\n",
            "(35, 30, 126)\n",
            "(51, 30, 126)\n",
            "(13, 30, 126)\n",
            "(22, 30, 126)\n",
            "(48, 30, 126)\n",
            "(10, 30, 126)\n",
            "(25, 30, 126)\n",
            "(98, 30, 126)\n",
            "(21, 30, 126)\n",
            "(43, 30, 126)\n",
            "(65, 30, 126)\n",
            "(4, 30, 126)\n",
            "(12, 30, 126)\n",
            "(47, 30, 126)\n",
            "(4, 30, 126)\n",
            "(33, 30, 126)\n",
            "(24, 30, 126)\n",
            "(12, 30, 126)\n",
            "(28, 30, 126)\n",
            "(13, 30, 126)\n",
            "(44, 30, 126)\n",
            "(63, 30, 126)\n",
            "(27, 30, 126)\n",
            "(21, 30, 126)\n",
            "(47, 30, 126)\n",
            "(9, 30, 126)\n",
            "(65, 30, 126)\n",
            "(72, 30, 126)\n",
            "(4, 30, 126)\n",
            "(5, 30, 126)\n",
            "(12, 30, 126)\n",
            "(36, 30, 126)\n",
            "(30, 30, 126)\n",
            "(46, 30, 126)\n",
            "(51, 30, 126)\n",
            "(39, 30, 126)\n",
            "(17, 30, 126)\n",
            "(57, 30, 126)\n",
            "(48, 30, 126)\n",
            "(14, 30, 126)\n",
            "(27, 30, 126)\n",
            "(15, 30, 126)\n",
            "(13, 30, 126)\n",
            "(65, 30, 126)\n",
            "(11, 30, 126)\n",
            "(11, 30, 126)\n",
            "(53, 30, 126)\n",
            "(9, 30, 126)\n",
            "(58, 30, 126)\n",
            "(54, 30, 126)\n",
            "(79, 30, 126)\n",
            "(2, 30, 126)\n",
            "(14, 30, 126)\n",
            "(61, 30, 126)\n",
            "(17, 30, 126)\n",
            "(35, 30, 126)\n",
            "(65, 30, 126)\n",
            "(42, 30, 126)\n",
            "(43, 30, 126)\n",
            "(21, 30, 126)\n",
            "(2, 30, 126)\n",
            "(58, 30, 126)\n",
            "(3, 30, 126)\n",
            "(3, 30, 126)\n",
            "(25, 30, 126)\n",
            "(58, 30, 126)\n",
            "(60, 30, 126)\n",
            "(76, 30, 126)\n",
            "(51, 30, 126)\n",
            "(76, 30, 126)\n",
            "(24, 30, 126)\n",
            "(32, 30, 126)\n",
            "(38, 30, 126)\n",
            "(52, 30, 126)\n",
            "(27, 30, 126)\n",
            "(101, 30, 126)\n",
            "(54, 30, 126)\n",
            "(56, 30, 126)\n",
            "(32, 30, 126)\n",
            "(8, 30, 126)\n",
            "(29, 30, 126)\n",
            "(56, 30, 126)\n",
            "(62, 30, 126)\n",
            "(50, 30, 126)\n",
            "(2, 30, 126)\n",
            "(46, 30, 126)\n",
            "(26, 30, 126)\n",
            "(11, 30, 126)\n",
            "(44, 30, 126)\n",
            "(18, 30, 126)\n",
            "(35, 30, 126)\n",
            "(33, 30, 126)\n",
            "(80, 30, 126)\n",
            "(32, 30, 126)\n",
            "(33, 30, 126)\n",
            "(36, 30, 126)\n",
            "(25, 30, 126)\n",
            "(37, 30, 126)\n",
            "(50, 30, 126)\n",
            "(65, 30, 126)\n",
            "(3, 30, 126)\n",
            "(26, 30, 126)\n",
            "(42, 30, 126)\n",
            "(10, 30, 126)\n",
            "(57, 30, 126)\n",
            "(9, 30, 126)\n",
            "(4, 30, 126)\n",
            "(87, 30, 126)\n",
            "(25, 30, 126)\n",
            "(19, 30, 126)\n",
            "(35, 30, 126)\n",
            "(25, 30, 126)\n",
            "(65, 30, 126)\n",
            "(47, 30, 126)\n",
            "(1, 30, 126)\n",
            "(9, 30, 126)\n",
            "(13, 30, 126)\n",
            "(23, 30, 126)\n",
            "(59, 30, 126)\n",
            "(47, 30, 126)\n",
            "(55, 30, 126)\n",
            "(35, 30, 126)\n",
            "(17, 30, 126)\n",
            "(56, 30, 126)\n",
            "(21, 30, 126)\n",
            "(21, 30, 126)\n",
            "(5, 30, 126)\n",
            "(12, 30, 126)\n",
            "(29, 30, 126)\n",
            "(57, 30, 126)\n",
            "(45, 30, 126)\n",
            "(31, 30, 126)\n",
            "(23, 30, 126)\n",
            "(38, 30, 126)\n",
            "(39, 30, 126)\n",
            "(32, 30, 126)\n",
            "(65, 30, 126)\n",
            "(74, 30, 126)\n",
            "(89, 30, 126)\n",
            "(65, 30, 126)\n",
            "(15, 30, 126)\n",
            "(6, 30, 126)\n",
            "(77, 30, 126)\n",
            "(3, 30, 126)\n",
            "(4, 30, 126)\n",
            "(73, 30, 126)\n",
            "(30, 30, 126)\n",
            "(33, 30, 126)\n",
            "(4, 30, 126)\n",
            "(70, 30, 126)\n",
            "(65, 30, 126)\n",
            "(57, 30, 126)\n",
            "(5, 30, 126)\n",
            "(7, 30, 126)\n",
            "(48, 30, 126)\n",
            "(43, 30, 126)\n",
            "(19, 30, 126)\n",
            "(21, 30, 126)\n",
            "(19, 30, 126)\n",
            "(45, 30, 126)\n",
            "(43, 30, 126)\n",
            "(42, 30, 126)\n",
            "(57, 30, 126)\n",
            "(60, 30, 126)\n",
            "(39, 30, 126)\n",
            "(48, 30, 126)\n",
            "(72, 30, 126)\n",
            "(16, 30, 126)\n",
            "(20, 30, 126)\n",
            "(15, 30, 126)\n",
            "(11, 30, 126)\n",
            "(15, 30, 126)\n",
            "(11, 30, 126)\n",
            "(41, 30, 126)\n",
            "(14, 30, 126)\n",
            "(16, 30, 126)\n",
            "(49, 30, 126)\n",
            "(100, 30, 126)\n",
            "(45, 30, 126)\n",
            "(80, 30, 126)\n",
            "(20, 30, 126)\n",
            "(49, 30, 126)\n",
            "(51, 30, 126)\n",
            "(42, 30, 126)\n",
            "(45, 30, 126)\n",
            "(6, 30, 126)\n",
            "(166, 30, 126)\n",
            "(31, 30, 126)\n",
            "(75, 30, 126)\n",
            "(71, 30, 126)\n",
            "(15, 30, 126)\n",
            "(68, 30, 126)\n",
            "(64, 30, 126)\n",
            "(62, 30, 126)\n",
            "(61, 30, 126)\n",
            "(6, 30, 126)\n",
            "(24, 30, 126)\n",
            "(31, 30, 126)\n",
            "(32, 30, 126)\n",
            "(26, 30, 126)\n",
            "(17, 30, 126)\n",
            "(19, 30, 126)\n",
            "(11, 30, 126)\n",
            "(22, 30, 126)\n",
            "(22, 30, 126)\n",
            "(52, 30, 126)\n",
            "(69, 30, 126)\n",
            "(35, 30, 126)\n",
            "(13, 30, 126)\n",
            "(56, 30, 126)\n",
            "(43, 30, 126)\n",
            "(21, 30, 126)\n",
            "(27, 30, 126)\n",
            "(29, 30, 126)\n",
            "(5, 30, 126)\n",
            "(90, 30, 126)\n",
            "(45, 30, 126)\n",
            "(98, 30, 126)\n",
            "(79, 30, 126)\n",
            "(14, 30, 126)\n",
            "(81, 30, 126)\n",
            "(48, 30, 126)\n",
            "(4, 30, 126)\n",
            "(9, 30, 126)\n",
            "(91, 30, 126)\n",
            "(34, 30, 126)\n",
            "(23, 30, 126)\n",
            "(27, 30, 126)\n",
            "(52, 30, 126)\n",
            "(58, 30, 126)\n",
            "(9, 30, 126)\n",
            "(3, 30, 126)\n",
            "(45, 30, 126)\n",
            "(13, 30, 126)\n",
            "(33, 30, 126)\n",
            "(20, 30, 126)\n",
            "(59, 30, 126)\n",
            "(72, 30, 126)\n",
            "(41, 30, 126)\n",
            "(30, 30, 126)\n",
            "(52, 30, 126)\n",
            "(39, 30, 126)\n",
            "(9, 30, 126)\n",
            "(65, 30, 126)\n",
            "(22, 30, 126)\n",
            "(15, 30, 126)\n",
            "(51, 30, 126)\n",
            "(17, 30, 126)\n",
            "(15, 30, 126)\n",
            "(14, 30, 126)\n",
            "(14, 30, 126)\n",
            "(64, 30, 126)\n",
            "(53, 30, 126)\n",
            "(30, 30, 126)\n",
            "(11, 30, 126)\n",
            "(52, 30, 126)\n",
            "(45, 30, 126)\n",
            "(94, 30, 126)\n",
            "(71, 30, 126)\n",
            "(23, 30, 126)\n",
            "(25, 30, 126)\n",
            "(4, 30, 126)\n",
            "(37, 30, 126)\n",
            "(68, 30, 126)\n",
            "(68, 30, 126)\n",
            "(72, 30, 126)\n",
            "(70, 30, 126)\n",
            "(6, 30, 126)\n",
            "(56, 30, 126)\n",
            "(63, 30, 126)\n",
            "(11, 30, 126)\n",
            "(12, 30, 126)\n",
            "(21, 30, 126)\n",
            "(72, 30, 126)\n",
            "(47, 30, 126)\n",
            "(31, 30, 126)\n",
            "(8, 30, 126)\n",
            "(46, 30, 126)\n",
            "(61, 30, 126)\n",
            "(71, 30, 126)\n",
            "(23, 30, 126)\n",
            "(2, 30, 126)\n",
            "(29, 30, 126)\n",
            "(16, 30, 126)\n",
            "(57, 30, 126)\n",
            "(79, 30, 126)\n",
            "(52, 30, 126)\n",
            "(58, 30, 126)\n",
            "(22, 30, 126)\n",
            "(2, 30, 126)\n",
            "(19, 30, 126)\n",
            "(18, 30, 126)\n",
            "(24, 30, 126)\n",
            "(24, 30, 126)\n",
            "(61, 30, 126)\n",
            "(58, 30, 126)\n",
            "(61, 30, 126)\n",
            "(2, 30, 126)\n",
            "(33, 30, 126)\n",
            "(8, 30, 126)\n",
            "(1, 30, 126)\n",
            "(1, 30, 126)\n",
            "(44, 30, 126)\n",
            "(43, 30, 126)\n",
            "(61, 30, 126)\n",
            "(90, 30, 126)\n",
            "(17, 30, 126)\n",
            "(48, 30, 126)\n",
            "(45, 30, 126)\n",
            "(26, 30, 126)\n",
            "(42, 30, 126)\n",
            "(45, 30, 126)\n",
            "(2, 30, 126)\n",
            "(27, 30, 126)\n",
            "(62, 30, 126)\n",
            "(24, 30, 126)\n",
            "(47, 30, 126)\n",
            "(36, 30, 126)\n",
            "(28, 30, 126)\n",
            "(38, 30, 126)\n",
            "(48, 30, 126)\n",
            "(15, 30, 126)\n",
            "(41, 30, 126)\n",
            "(64, 30, 126)\n",
            "(17, 30, 126)\n",
            "(60, 30, 126)\n",
            "(52, 30, 126)\n",
            "(14, 30, 126)\n",
            "(62, 30, 126)\n",
            "(76, 30, 126)\n",
            "(11, 30, 126)\n",
            "(19, 30, 126)\n",
            "(19, 30, 126)\n",
            "(30, 30, 126)\n",
            "(38, 30, 126)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Conv1D, GlobalAveragePooling1D, LayerNormalization, MultiHeadAttention, Add\n",
        "from tensorflow.keras.optimizers import AdamW\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "# Folder path where all the .npy files are stored\n",
        "npy_folder_path = '/content/drive/MyDrive/Semester 5/DL/Project/processed_landmarks_from_csv'\n",
        "\n",
        "# Dictionary to store the data grouped by labels\n",
        "label_video_dict = {}\n",
        "\n",
        "# Loop through the folder and group data by labels\n",
        "for npy_file in os.listdir(npy_folder_path):\n",
        "    if npy_file.endswith('.npy'):\n",
        "        # Load the data\n",
        "        data = np.load(os.path.join(npy_folder_path, npy_file))\n",
        "\n",
        "        print(data.shape)\n",
        "        # Check if the data has a valid shape\n",
        "        if data.shape == (0,):\n",
        "            print(f\"Skipping file {npy_file} due to invalid shape {data.shape}\")\n",
        "            continue  # Skip the current file\n",
        "\n",
        "        # Rescale the data\n",
        "        reshaped_data = (data - np.min(data)) / (np.max(data) - np.min(data))\n",
        "\n",
        "        # Extract the label from the file name (assuming label is part of the file name)\n",
        "        label = npy_file.split('_')[0:-1]  # Adjust as per naming convention\n",
        "        label = '_'.join(label)  # Join the label parts if there are underscores\n",
        "\n",
        "        # Append the reshaped data under the corresponding label\n",
        "        if label not in label_video_dict:\n",
        "            label_video_dict[label] = []\n",
        "        label_video_dict[label].append(reshaped_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CM12rmIEg-lt",
        "outputId": "554ac4a3-041e-47b2-ebc5-c84c4f8ea364"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique labels: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
            " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
            " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
            " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n",
            " 96 97 98 99]\n",
            "Number of distinct labels: 100\n",
            "X_train shape: (31849, 30, 126)\n",
            "y_train shape: (31849, 100)\n",
            "X_test shape: (7963, 30, 126)\n",
            "y_test shape: (7963, 100)\n"
          ]
        }
      ],
      "source": [
        "# Prepare data and labels for encoding\n",
        "all_data = []\n",
        "all_labels = []\n",
        "\n",
        "for label, videos in label_video_dict.items():\n",
        "    for video in videos:\n",
        "        all_data.append(video)  # Each video is already an array of shape (num_sequences, 30, 126)\n",
        "        all_labels.extend([label] * video.shape[0])  # Extend with the label for each sequence in the video\n",
        "\n",
        "# Concatenate the list of arrays along the first axis\n",
        "X = np.concatenate(all_data, axis=0)  # This will combine all the sequences\n",
        "# Encode the labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(all_labels)\n",
        "\n",
        "# Check unique labels\n",
        "y_vals = np.unique(y_encoded)\n",
        "num_classes = len(y_vals)\n",
        "print(\"Unique labels:\", y_vals)\n",
        "print(\"Number of distinct labels:\", num_classes)\n",
        "\n",
        "# One-hot encode the labels\n",
        "y = to_categorical(y_encoded, num_classes=num_classes)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,  # 20% of the data for testing\n",
        "    random_state=42,  # For reproducibility\n",
        "    stratify=y_encoded  # Ensures the same distribution of classes in both sets\n",
        ")\n",
        "\n",
        "# Print the shapes for verification\n",
        "print(\"X_train shape:\", X_train.shape)  # Should match the number of sequences in training\n",
        "print(\"y_train shape:\", y_train.shape)  # Should match X_train\n",
        "print(\"X_test shape:\", X_test.shape)    # Should match the number of sequences in testing\n",
        "print(\"y_test shape:\", y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPX7vo9Ng-lu",
        "outputId": "c7024a2d-e973-4bf5-9fc1-9f3d6f1b6f7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique labels: ['accident' 'apple' 'argue' 'bad' 'balance' 'bar' 'basketball' 'because'\n",
            " 'bed' 'before' 'bird' 'black' 'blanket' 'bowling' 'brother' 'call'\n",
            " 'candy' 'champion' 'change' 'cheat' 'check' 'cold' 'computer' 'convince'\n",
            " 'cool' 'corn' 'cousin' 'cry' 'dark' 'daughter' 'deaf' 'delay' 'delicious'\n",
            " 'doctor' 'dog' 'drink' 'environment' 'example' 'family' 'far' 'fat'\n",
            " 'fish' 'full' 'give' 'go' 'good' 'government' 'graduate' 'help' 'hot'\n",
            " 'interest' 'language' 'last' 'later' 'laugh' 'leave' 'letter' 'like'\n",
            " 'man' 'many' 'mother' 'move' 'no' 'orange' 'order' 'perspective' 'pizza'\n",
            " 'play' 'room' 'sandwich' 'score' 'secretary' 'shirt' 'short' 'silly'\n",
            " 'snow' 'son' 'soon' 'study' 'sweet' 'take' 'tall' 'tell' 'thanksgiving'\n",
            " 'theory' 'thin' 'thursday' 'trade' 'wait' 'walk' 'what' 'white' 'who'\n",
            " 'why' 'woman' 'work' 'write' 'year' 'yes' 'yesterday']\n",
            "Number of distinct encoded: 100\n"
          ]
        }
      ],
      "source": [
        "types_labels = np.unique(all_labels)\n",
        "print(\"Unique labels:\", types_labels)\n",
        "print(\"Number of distinct encoded:\", len(types_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZIRSx6fdg-lu",
        "outputId": "f1d2a82c-7439-4f6c-89a7-6c75aacc4f11"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m\n",
              "\n",
              " input_layer (\u001b[38;5;33mInputLayer\u001b[0m)   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m126\u001b[0m)                      \u001b[38;5;34m0\u001b[0m  -                      \n",
              "\n",
              " dense (\u001b[38;5;33mDense\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m126\u001b[0m)                 \u001b[38;5;34m16,002\u001b[0m  input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n",
              " conv1d (\u001b[38;5;33mConv1D\u001b[0m)            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  \u001b[38;5;34m24,256\u001b[0m  dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            \n",
              "\n",
              " batch_normalization        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m64\u001b[0m)                     \u001b[38;5;34m256\u001b[0m  conv1d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " multi_head_attention       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  \u001b[38;5;34m66,368\u001b[0m  batch_normalization[\u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mMultiHeadAttention\u001b[0m)                                               batch_normalization[\u001b[38;5;34m0\u001b[0m \n",
              "\n",
              " dropout_1 (\u001b[38;5;33mDropout\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m64\u001b[0m)                       \u001b[38;5;34m0\u001b[0m  multi_head_attention[\u001b[38;5;34m\u001b[0m \n",
              "\n",
              " add (\u001b[38;5;33mAdd\u001b[0m)                  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m64\u001b[0m)                       \u001b[38;5;34m0\u001b[0m  batch_normalization[\u001b[38;5;34m0\u001b[0m \n",
              "                                                                    dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              "\n",
              " layer_normalization        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m64\u001b[0m)                     \u001b[38;5;34m128\u001b[0m  add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              \n",
              " (\u001b[38;5;33mLayerNormalization\u001b[0m)                                                                      \n",
              "\n",
              " dense_1 (\u001b[38;5;33mDense\u001b[0m)            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m8,320\u001b[0m  layer_normalization[\u001b[38;5;34m0\u001b[0m \n",
              "\n",
              " dense_2 (\u001b[38;5;33mDense\u001b[0m)            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m8,256\u001b[0m  dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n",
              "\n",
              " dropout_2 (\u001b[38;5;33mDropout\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m64\u001b[0m)                       \u001b[38;5;34m0\u001b[0m  dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n",
              "\n",
              " add_1 (\u001b[38;5;33mAdd\u001b[0m)                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m64\u001b[0m)                       \u001b[38;5;34m0\u001b[0m  layer_normalization[\u001b[38;5;34m0\u001b[0m \n",
              "                                                                    dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              "\n",
              " layer_normalization_1      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m64\u001b[0m)                     \u001b[38;5;34m128\u001b[0m  add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            \n",
              " (\u001b[38;5;33mLayerNormalization\u001b[0m)                                                                      \n",
              "\n",
              " multi_head_attention_1     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  \u001b[38;5;34m66,368\u001b[0m  layer_normalization_1 \n",
              " (\u001b[38;5;33mMultiHeadAttention\u001b[0m)                                               layer_normalization_1 \n",
              "\n",
              " dropout_4 (\u001b[38;5;33mDropout\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m64\u001b[0m)                       \u001b[38;5;34m0\u001b[0m  multi_head_attention_ \n",
              "\n",
              " add_2 (\u001b[38;5;33mAdd\u001b[0m)                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m64\u001b[0m)                       \u001b[38;5;34m0\u001b[0m  layer_normalization_1 \n",
              "                                                                    dropout_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              "\n",
              " layer_normalization_2      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m64\u001b[0m)                     \u001b[38;5;34m128\u001b[0m  add_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            \n",
              " (\u001b[38;5;33mLayerNormalization\u001b[0m)                                                                      \n",
              "\n",
              " dense_3 (\u001b[38;5;33mDense\u001b[0m)            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m8,320\u001b[0m  layer_normalization_2 \n",
              "\n",
              " dense_4 (\u001b[38;5;33mDense\u001b[0m)            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m8,256\u001b[0m  dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n",
              "\n",
              " dropout_5 (\u001b[38;5;33mDropout\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m64\u001b[0m)                       \u001b[38;5;34m0\u001b[0m  dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n",
              "\n",
              " add_3 (\u001b[38;5;33mAdd\u001b[0m)                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m64\u001b[0m)                       \u001b[38;5;34m0\u001b[0m  layer_normalization_2 \n",
              "                                                                    dropout_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              "\n",
              " layer_normalization_3      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m64\u001b[0m)                     \u001b[38;5;34m128\u001b[0m  add_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            \n",
              " (\u001b[38;5;33mLayerNormalization\u001b[0m)                                                                      \n",
              "\n",
              " global_average_pooling1d   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  layer_normalization_3 \n",
              " (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)                                                                  \n",
              "\n",
              " dense_5 (\u001b[38;5;33mDense\u001b[0m)            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                      \u001b[38;5;34m6,500\u001b[0m  global_average_poolin \n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\"> Layer (type)              </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">        Param # </span><span style=\"font-weight: bold\"> Connected to           </span>\n",
              "\n",
              " input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                      \n",
              "\n",
              " dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">16,002</span>  input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              " conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">24,256</span>  dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            \n",
              "\n",
              " batch_normalization        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " multi_head_attention       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">66,368</span>  batch_normalization[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)                                               batch_normalization[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              "\n",
              " dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                       <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  multi_head_attention[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n",
              "\n",
              " add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                       <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              "                                                                    dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              "\n",
              " layer_normalization        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>  add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                                                                      \n",
              "\n",
              " dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span>  layer_normalization[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              "\n",
              " dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span>  dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n",
              "\n",
              " dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                       <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n",
              "\n",
              " add_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                       <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  layer_normalization[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              "                                                                    dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              "\n",
              " layer_normalization_1      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>  add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                                                                      \n",
              "\n",
              " multi_head_attention_1     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">66,368</span>  layer_normalization_1 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)                                               layer_normalization_1 \n",
              "\n",
              " dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                       <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  multi_head_attention_ \n",
              "\n",
              " add_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                       <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  layer_normalization_1 \n",
              "                                                                    dropout_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              "\n",
              " layer_normalization_2      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>  add_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                                                                      \n",
              "\n",
              " dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span>  layer_normalization_2 \n",
              "\n",
              " dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span>  dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n",
              "\n",
              " dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                       <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n",
              "\n",
              " add_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                       <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  layer_normalization_2 \n",
              "                                                                    dropout_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              "\n",
              " layer_normalization_3      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>  add_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                                                                      \n",
              "\n",
              " global_average_pooling1d   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  layer_normalization_3 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)                                                                  \n",
              "\n",
              " dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">6,500</span>  global_average_poolin \n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m213,414\u001b[0m (833.65 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">213,414</span> (833.65 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m213,286\u001b[0m (833.15 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">213,286</span> (833.15 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m128\u001b[0m (512.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> (512.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.1299 - loss: 4.2004\n",
            "Epoch 1: val_loss improved from inf to 2.74893, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_01.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 15ms/step - accuracy: 0.1300 - loss: 4.1998 - val_accuracy: 0.3685 - val_loss: 2.7489 - learning_rate: 5.0000e-04\n",
            "Epoch 2/150\n",
            "\u001b[1m987/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5128 - loss: 2.3183\n",
            "Epoch 2: val_loss improved from 2.74893 to 2.11870, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_02.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.5133 - loss: 2.3159 - val_accuracy: 0.5067 - val_loss: 2.1187 - learning_rate: 5.0000e-04\n",
            "Epoch 3/150\n",
            "\u001b[1m992/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7065 - loss: 1.5129\n",
            "Epoch 3: val_loss improved from 2.11870 to 1.36817, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_03.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.7067 - loss: 1.5124 - val_accuracy: 0.7187 - val_loss: 1.3682 - learning_rate: 5.0000e-04\n",
            "Epoch 4/150\n",
            "\u001b[1m988/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7991 - loss: 1.1398\n",
            "Epoch 4: val_loss improved from 1.36817 to 1.34581, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_04.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.7993 - loss: 1.1392 - val_accuracy: 0.7069 - val_loss: 1.3458 - learning_rate: 5.0000e-04\n",
            "Epoch 5/150\n",
            "\u001b[1m988/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8559 - loss: 0.9164\n",
            "Epoch 5: val_loss improved from 1.34581 to 0.86539, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_05.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.8560 - loss: 0.9162 - val_accuracy: 0.8542 - val_loss: 0.8654 - learning_rate: 5.0000e-04\n",
            "Epoch 6/150\n",
            "\u001b[1m990/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8757 - loss: 0.8260\n",
            "Epoch 6: val_loss improved from 0.86539 to 0.73143, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_06.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.8757 - loss: 0.8258 - val_accuracy: 0.8897 - val_loss: 0.7314 - learning_rate: 5.0000e-04\n",
            "Epoch 7/150\n",
            "\u001b[1m985/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8971 - loss: 0.7226\n",
            "Epoch 7: val_loss improved from 0.73143 to 0.72420, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_07.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.8971 - loss: 0.7224 - val_accuracy: 0.8881 - val_loss: 0.7242 - learning_rate: 5.0000e-04\n",
            "Epoch 8/150\n",
            "\u001b[1m994/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9050 - loss: 0.6685\n",
            "Epoch 8: val_loss improved from 0.72420 to 0.72040, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_08.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9050 - loss: 0.6685 - val_accuracy: 0.8754 - val_loss: 0.7204 - learning_rate: 5.0000e-04\n",
            "Epoch 9/150\n",
            "\u001b[1m992/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9124 - loss: 0.6189\n",
            "Epoch 9: val_loss improved from 0.72040 to 0.65867, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_09.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9124 - loss: 0.6189 - val_accuracy: 0.8936 - val_loss: 0.6587 - learning_rate: 5.0000e-04\n",
            "Epoch 10/150\n",
            "\u001b[1m987/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9224 - loss: 0.5654\n",
            "Epoch 10: val_loss did not improve from 0.65867\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.9224 - loss: 0.5654 - val_accuracy: 0.8560 - val_loss: 0.7583 - learning_rate: 5.0000e-04\n",
            "Epoch 11/150\n",
            "\u001b[1m987/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9235 - loss: 0.5525\n",
            "Epoch 11: val_loss did not improve from 0.65867\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9235 - loss: 0.5524 - val_accuracy: 0.8356 - val_loss: 0.8015 - learning_rate: 5.0000e-04\n",
            "Epoch 12/150\n",
            "\u001b[1m995/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9257 - loss: 0.5231\n",
            "Epoch 12: val_loss improved from 0.65867 to 0.57684, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_12.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9257 - loss: 0.5231 - val_accuracy: 0.8983 - val_loss: 0.5768 - learning_rate: 5.0000e-04\n",
            "Epoch 13/150\n",
            "\u001b[1m995/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9344 - loss: 0.4764\n",
            "Epoch 13: val_loss did not improve from 0.57684\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.9343 - loss: 0.4765 - val_accuracy: 0.8954 - val_loss: 0.5841 - learning_rate: 5.0000e-04\n",
            "Epoch 14/150\n",
            "\u001b[1m985/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9284 - loss: 0.4845\n",
            "Epoch 14: val_loss improved from 0.57684 to 0.45574, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_14.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9284 - loss: 0.4843 - val_accuracy: 0.9318 - val_loss: 0.4557 - learning_rate: 5.0000e-04\n",
            "Epoch 15/150\n",
            "\u001b[1m988/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9335 - loss: 0.4588\n",
            "Epoch 15: val_loss did not improve from 0.45574\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9335 - loss: 0.4588 - val_accuracy: 0.8278 - val_loss: 0.7565 - learning_rate: 5.0000e-04\n",
            "Epoch 16/150\n",
            "\u001b[1m991/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9342 - loss: 0.4425\n",
            "Epoch 16: val_loss did not improve from 0.45574\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9342 - loss: 0.4425 - val_accuracy: 0.8744 - val_loss: 0.6117 - learning_rate: 5.0000e-04\n",
            "Epoch 17/150\n",
            "\u001b[1m994/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9420 - loss: 0.4061\n",
            "Epoch 17: val_loss did not improve from 0.45574\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9420 - loss: 0.4061 - val_accuracy: 0.8920 - val_loss: 0.5729 - learning_rate: 5.0000e-04\n",
            "Epoch 18/150\n",
            "\u001b[1m989/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9412 - loss: 0.3986\n",
            "Epoch 18: val_loss did not improve from 0.45574\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9412 - loss: 0.3987 - val_accuracy: 0.8930 - val_loss: 0.5447 - learning_rate: 5.0000e-04\n",
            "Epoch 19/150\n",
            "\u001b[1m987/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9401 - loss: 0.3926\n",
            "Epoch 19: val_loss did not improve from 0.45574\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.9401 - loss: 0.3925 - val_accuracy: 0.8966 - val_loss: 0.5152 - learning_rate: 5.0000e-04\n",
            "Epoch 20/150\n",
            "\u001b[1m991/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9596 - loss: 0.3127\n",
            "Epoch 20: val_loss improved from 0.45574 to 0.30631, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_20.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9596 - loss: 0.3126 - val_accuracy: 0.9591 - val_loss: 0.3063 - learning_rate: 5.0000e-05\n",
            "Epoch 21/150\n",
            "\u001b[1m987/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9637 - loss: 0.2915\n",
            "Epoch 21: val_loss improved from 0.30631 to 0.30247, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_21.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9638 - loss: 0.2914 - val_accuracy: 0.9597 - val_loss: 0.3025 - learning_rate: 5.0000e-05\n",
            "Epoch 22/150\n",
            "\u001b[1m990/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9642 - loss: 0.2867\n",
            "Epoch 22: val_loss improved from 0.30247 to 0.29757, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_22.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - accuracy: 0.9642 - loss: 0.2867 - val_accuracy: 0.9607 - val_loss: 0.2976 - learning_rate: 5.0000e-05\n",
            "Epoch 23/150\n",
            "\u001b[1m993/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9646 - loss: 0.2799\n",
            "Epoch 23: val_loss improved from 0.29757 to 0.29148, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_23.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9646 - loss: 0.2799 - val_accuracy: 0.9606 - val_loss: 0.2915 - learning_rate: 5.0000e-05\n",
            "Epoch 24/150\n",
            "\u001b[1m992/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9672 - loss: 0.2692\n",
            "Epoch 24: val_loss improved from 0.29148 to 0.28919, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_24.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9672 - loss: 0.2693 - val_accuracy: 0.9613 - val_loss: 0.2892 - learning_rate: 5.0000e-05\n",
            "Epoch 25/150\n",
            "\u001b[1m995/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9647 - loss: 0.2772\n",
            "Epoch 25: val_loss improved from 0.28919 to 0.28554, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_25.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9647 - loss: 0.2772 - val_accuracy: 0.9602 - val_loss: 0.2855 - learning_rate: 5.0000e-05\n",
            "Epoch 26/150\n",
            "\u001b[1m994/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9671 - loss: 0.2646\n",
            "Epoch 26: val_loss improved from 0.28554 to 0.28222, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_26.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.9671 - loss: 0.2646 - val_accuracy: 0.9619 - val_loss: 0.2822 - learning_rate: 5.0000e-05\n",
            "Epoch 27/150\n",
            "\u001b[1m990/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9659 - loss: 0.2657\n",
            "Epoch 27: val_loss improved from 0.28222 to 0.27862, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_27.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.9659 - loss: 0.2657 - val_accuracy: 0.9619 - val_loss: 0.2786 - learning_rate: 5.0000e-05\n",
            "Epoch 28/150\n",
            "\u001b[1m995/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9659 - loss: 0.2625\n",
            "Epoch 28: val_loss did not improve from 0.27862\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9659 - loss: 0.2625 - val_accuracy: 0.9606 - val_loss: 0.2803 - learning_rate: 5.0000e-05\n",
            "Epoch 29/150\n",
            "\u001b[1m995/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9674 - loss: 0.2579\n",
            "Epoch 29: val_loss improved from 0.27862 to 0.27633, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_29.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9674 - loss: 0.2579 - val_accuracy: 0.9603 - val_loss: 0.2763 - learning_rate: 5.0000e-05\n",
            "Epoch 30/150\n",
            "\u001b[1m986/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9668 - loss: 0.2570\n",
            "Epoch 30: val_loss improved from 0.27633 to 0.27414, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_30.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9667 - loss: 0.2570 - val_accuracy: 0.9626 - val_loss: 0.2741 - learning_rate: 5.0000e-05\n",
            "Epoch 31/150\n",
            "\u001b[1m992/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9685 - loss: 0.2507\n",
            "Epoch 31: val_loss improved from 0.27414 to 0.27255, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_31.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - accuracy: 0.9685 - loss: 0.2507 - val_accuracy: 0.9613 - val_loss: 0.2726 - learning_rate: 5.0000e-05\n",
            "Epoch 32/150\n",
            "\u001b[1m995/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9665 - loss: 0.2553\n",
            "Epoch 32: val_loss improved from 0.27255 to 0.27063, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_32.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9665 - loss: 0.2553 - val_accuracy: 0.9606 - val_loss: 0.2706 - learning_rate: 5.0000e-05\n",
            "Epoch 33/150\n",
            "\u001b[1m992/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9676 - loss: 0.2493\n",
            "Epoch 33: val_loss improved from 0.27063 to 0.26689, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_33.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.9676 - loss: 0.2493 - val_accuracy: 0.9619 - val_loss: 0.2669 - learning_rate: 5.0000e-05\n",
            "Epoch 34/150\n",
            "\u001b[1m994/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9678 - loss: 0.2448\n",
            "Epoch 34: val_loss improved from 0.26689 to 0.26661, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_34.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9678 - loss: 0.2448 - val_accuracy: 0.9599 - val_loss: 0.2666 - learning_rate: 5.0000e-05\n",
            "Epoch 35/150\n",
            "\u001b[1m987/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9660 - loss: 0.2520\n",
            "Epoch 35: val_loss improved from 0.26661 to 0.26448, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_35.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9660 - loss: 0.2520 - val_accuracy: 0.9621 - val_loss: 0.2645 - learning_rate: 5.0000e-05\n",
            "Epoch 36/150\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9680 - loss: 0.2429\n",
            "Epoch 36: val_loss improved from 0.26448 to 0.26264, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_36.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - accuracy: 0.9680 - loss: 0.2429 - val_accuracy: 0.9611 - val_loss: 0.2626 - learning_rate: 5.0000e-05\n",
            "Epoch 37/150\n",
            "\u001b[1m994/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9672 - loss: 0.2460\n",
            "Epoch 37: val_loss improved from 0.26264 to 0.26148, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_37.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9672 - loss: 0.2460 - val_accuracy: 0.9603 - val_loss: 0.2615 - learning_rate: 5.0000e-05\n",
            "Epoch 38/150\n",
            "\u001b[1m987/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9657 - loss: 0.2468\n",
            "Epoch 38: val_loss did not improve from 0.26148\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9657 - loss: 0.2467 - val_accuracy: 0.9614 - val_loss: 0.2635 - learning_rate: 5.0000e-05\n",
            "Epoch 39/150\n",
            "\u001b[1m984/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9668 - loss: 0.2443\n",
            "Epoch 39: val_loss improved from 0.26148 to 0.26125, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_39.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9668 - loss: 0.2442 - val_accuracy: 0.9614 - val_loss: 0.2613 - learning_rate: 5.0000e-05\n",
            "Epoch 40/150\n",
            "\u001b[1m992/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9692 - loss: 0.2312\n",
            "Epoch 40: val_loss improved from 0.26125 to 0.25890, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_40.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9692 - loss: 0.2312 - val_accuracy: 0.9607 - val_loss: 0.2589 - learning_rate: 5.0000e-05\n",
            "Epoch 41/150\n",
            "\u001b[1m988/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9664 - loss: 0.2423\n",
            "Epoch 41: val_loss improved from 0.25890 to 0.25572, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_41.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9664 - loss: 0.2423 - val_accuracy: 0.9616 - val_loss: 0.2557 - learning_rate: 5.0000e-05\n",
            "Epoch 42/150\n",
            "\u001b[1m987/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9693 - loss: 0.2292\n",
            "Epoch 42: val_loss improved from 0.25572 to 0.25445, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_42.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9693 - loss: 0.2293 - val_accuracy: 0.9616 - val_loss: 0.2544 - learning_rate: 5.0000e-05\n",
            "Epoch 43/150\n",
            "\u001b[1m985/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9671 - loss: 0.2368\n",
            "Epoch 43: val_loss did not improve from 0.25445\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9671 - loss: 0.2367 - val_accuracy: 0.9621 - val_loss: 0.2558 - learning_rate: 5.0000e-05\n",
            "Epoch 44/150\n",
            "\u001b[1m989/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9676 - loss: 0.2354\n",
            "Epoch 44: val_loss did not improve from 0.25445\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9676 - loss: 0.2354 - val_accuracy: 0.9622 - val_loss: 0.2551 - learning_rate: 5.0000e-05\n",
            "Epoch 45/150\n",
            "\u001b[1m990/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9675 - loss: 0.2326\n",
            "Epoch 45: val_loss improved from 0.25445 to 0.25182, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_45.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9675 - loss: 0.2326 - val_accuracy: 0.9622 - val_loss: 0.2518 - learning_rate: 5.0000e-05\n",
            "Epoch 46/150\n",
            "\u001b[1m993/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9680 - loss: 0.2307\n",
            "Epoch 46: val_loss did not improve from 0.25182\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9680 - loss: 0.2307 - val_accuracy: 0.9625 - val_loss: 0.2533 - learning_rate: 5.0000e-05\n",
            "Epoch 47/150\n",
            "\u001b[1m989/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9690 - loss: 0.2308\n",
            "Epoch 47: val_loss improved from 0.25182 to 0.25049, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_47.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9690 - loss: 0.2308 - val_accuracy: 0.9625 - val_loss: 0.2505 - learning_rate: 5.0000e-05\n",
            "Epoch 48/150\n",
            "\u001b[1m984/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9695 - loss: 0.2244\n",
            "Epoch 48: val_loss improved from 0.25049 to 0.25030, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_48.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9695 - loss: 0.2244 - val_accuracy: 0.9616 - val_loss: 0.2503 - learning_rate: 5.0000e-05\n",
            "Epoch 49/150\n",
            "\u001b[1m986/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9700 - loss: 0.2200\n",
            "Epoch 49: val_loss improved from 0.25030 to 0.24821, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_49.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9700 - loss: 0.2200 - val_accuracy: 0.9628 - val_loss: 0.2482 - learning_rate: 5.0000e-05\n",
            "Epoch 50/150\n",
            "\u001b[1m988/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9688 - loss: 0.2251\n",
            "Epoch 50: val_loss did not improve from 0.24821\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9688 - loss: 0.2251 - val_accuracy: 0.9608 - val_loss: 0.2496 - learning_rate: 5.0000e-05\n",
            "Epoch 51/150\n",
            "\u001b[1m994/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9703 - loss: 0.2115\n",
            "Epoch 51: val_loss did not improve from 0.24821\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9703 - loss: 0.2115 - val_accuracy: 0.9613 - val_loss: 0.2484 - learning_rate: 5.0000e-05\n",
            "Epoch 52/150\n",
            "\u001b[1m984/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9698 - loss: 0.2193\n",
            "Epoch 52: val_loss improved from 0.24821 to 0.24592, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_52.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9698 - loss: 0.2193 - val_accuracy: 0.9618 - val_loss: 0.2459 - learning_rate: 5.0000e-05\n",
            "Epoch 53/150\n",
            "\u001b[1m991/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9696 - loss: 0.2200\n",
            "Epoch 53: val_loss improved from 0.24592 to 0.24395, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_53.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.9696 - loss: 0.2200 - val_accuracy: 0.9626 - val_loss: 0.2439 - learning_rate: 5.0000e-05\n",
            "Epoch 54/150\n",
            "\u001b[1m985/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9703 - loss: 0.2153\n",
            "Epoch 54: val_loss did not improve from 0.24395\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.9703 - loss: 0.2153 - val_accuracy: 0.9625 - val_loss: 0.2517 - learning_rate: 5.0000e-05\n",
            "Epoch 55/150\n",
            "\u001b[1m993/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9705 - loss: 0.2129\n",
            "Epoch 55: val_loss improved from 0.24395 to 0.24259, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_55.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9705 - loss: 0.2129 - val_accuracy: 0.9625 - val_loss: 0.2426 - learning_rate: 5.0000e-05\n",
            "Epoch 56/150\n",
            "\u001b[1m992/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9701 - loss: 0.2127\n",
            "Epoch 56: val_loss did not improve from 0.24259\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9701 - loss: 0.2127 - val_accuracy: 0.9623 - val_loss: 0.2428 - learning_rate: 5.0000e-05\n",
            "Epoch 57/150\n",
            "\u001b[1m990/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9695 - loss: 0.2162\n",
            "Epoch 57: val_loss improved from 0.24259 to 0.24031, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_57.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9695 - loss: 0.2162 - val_accuracy: 0.9621 - val_loss: 0.2403 - learning_rate: 5.0000e-05\n",
            "Epoch 58/150\n",
            "\u001b[1m983/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9725 - loss: 0.1999\n",
            "Epoch 58: val_loss improved from 0.24031 to 0.23782, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_58.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9724 - loss: 0.2001 - val_accuracy: 0.9631 - val_loss: 0.2378 - learning_rate: 5.0000e-05\n",
            "Epoch 59/150\n",
            "\u001b[1m984/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9703 - loss: 0.2078\n",
            "Epoch 59: val_loss improved from 0.23782 to 0.23668, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_59.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9703 - loss: 0.2079 - val_accuracy: 0.9618 - val_loss: 0.2367 - learning_rate: 5.0000e-05\n",
            "Epoch 60/150\n",
            "\u001b[1m988/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9717 - loss: 0.2045\n",
            "Epoch 60: val_loss did not improve from 0.23668\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.9717 - loss: 0.2045 - val_accuracy: 0.9611 - val_loss: 0.2394 - learning_rate: 5.0000e-05\n",
            "Epoch 61/150\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9701 - loss: 0.2106\n",
            "Epoch 61: val_loss improved from 0.23668 to 0.23550, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_61.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9701 - loss: 0.2106 - val_accuracy: 0.9628 - val_loss: 0.2355 - learning_rate: 5.0000e-05\n",
            "Epoch 62/150\n",
            "\u001b[1m989/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9707 - loss: 0.2034\n",
            "Epoch 62: val_loss improved from 0.23550 to 0.23346, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_62.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.9707 - loss: 0.2035 - val_accuracy: 0.9631 - val_loss: 0.2335 - learning_rate: 5.0000e-05\n",
            "Epoch 63/150\n",
            "\u001b[1m983/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9702 - loss: 0.2050\n",
            "Epoch 63: val_loss did not improve from 0.23346\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9702 - loss: 0.2051 - val_accuracy: 0.9627 - val_loss: 0.2370 - learning_rate: 5.0000e-05\n",
            "Epoch 64/150\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9719 - loss: 0.2010\n",
            "Epoch 64: val_loss improved from 0.23346 to 0.23280, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_64.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9719 - loss: 0.2010 - val_accuracy: 0.9640 - val_loss: 0.2328 - learning_rate: 5.0000e-05\n",
            "Epoch 65/150\n",
            "\u001b[1m990/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9703 - loss: 0.2071\n",
            "Epoch 65: val_loss improved from 0.23280 to 0.23134, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_65.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9703 - loss: 0.2071 - val_accuracy: 0.9630 - val_loss: 0.2313 - learning_rate: 5.0000e-05\n",
            "Epoch 66/150\n",
            "\u001b[1m986/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9699 - loss: 0.2082\n",
            "Epoch 66: val_loss did not improve from 0.23134\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9699 - loss: 0.2082 - val_accuracy: 0.9630 - val_loss: 0.2344 - learning_rate: 5.0000e-05\n",
            "Epoch 67/150\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9702 - loss: 0.2016\n",
            "Epoch 67: val_loss did not improve from 0.23134\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9702 - loss: 0.2016 - val_accuracy: 0.9630 - val_loss: 0.2316 - learning_rate: 5.0000e-05\n",
            "Epoch 68/150\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9713 - loss: 0.1997\n",
            "Epoch 68: val_loss improved from 0.23134 to 0.22745, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_68.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9713 - loss: 0.1997 - val_accuracy: 0.9621 - val_loss: 0.2275 - learning_rate: 5.0000e-05\n",
            "Epoch 69/150\n",
            "\u001b[1m992/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9698 - loss: 0.2015\n",
            "Epoch 69: val_loss did not improve from 0.22745\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9698 - loss: 0.2015 - val_accuracy: 0.9630 - val_loss: 0.2324 - learning_rate: 5.0000e-05\n",
            "Epoch 70/150\n",
            "\u001b[1m989/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9713 - loss: 0.1988\n",
            "Epoch 70: val_loss improved from 0.22745 to 0.22610, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_70.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.9713 - loss: 0.1988 - val_accuracy: 0.9637 - val_loss: 0.2261 - learning_rate: 5.0000e-05\n",
            "Epoch 71/150\n",
            "\u001b[1m990/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9702 - loss: 0.2003\n",
            "Epoch 71: val_loss did not improve from 0.22610\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9702 - loss: 0.2003 - val_accuracy: 0.9625 - val_loss: 0.2287 - learning_rate: 5.0000e-05\n",
            "Epoch 72/150\n",
            "\u001b[1m986/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9711 - loss: 0.1937\n",
            "Epoch 72: val_loss did not improve from 0.22610\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9710 - loss: 0.1938 - val_accuracy: 0.9630 - val_loss: 0.2323 - learning_rate: 5.0000e-05\n",
            "Epoch 73/150\n",
            "\u001b[1m993/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9716 - loss: 0.1909\n",
            "Epoch 73: val_loss improved from 0.22610 to 0.22313, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_73.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9716 - loss: 0.1909 - val_accuracy: 0.9640 - val_loss: 0.2231 - learning_rate: 5.0000e-05\n",
            "Epoch 74/150\n",
            "\u001b[1m983/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9724 - loss: 0.1931\n",
            "Epoch 74: val_loss did not improve from 0.22313\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9723 - loss: 0.1932 - val_accuracy: 0.9616 - val_loss: 0.2276 - learning_rate: 5.0000e-05\n",
            "Epoch 75/150\n",
            "\u001b[1m987/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9720 - loss: 0.1942\n",
            "Epoch 75: val_loss improved from 0.22313 to 0.22244, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_75.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9720 - loss: 0.1942 - val_accuracy: 0.9638 - val_loss: 0.2224 - learning_rate: 5.0000e-05\n",
            "Epoch 76/150\n",
            "\u001b[1m987/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9712 - loss: 0.1951\n",
            "Epoch 76: val_loss did not improve from 0.22244\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9712 - loss: 0.1951 - val_accuracy: 0.9623 - val_loss: 0.2243 - learning_rate: 5.0000e-05\n",
            "Epoch 77/150\n",
            "\u001b[1m989/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9706 - loss: 0.1941\n",
            "Epoch 77: val_loss improved from 0.22244 to 0.22145, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_77.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.9706 - loss: 0.1941 - val_accuracy: 0.9637 - val_loss: 0.2215 - learning_rate: 5.0000e-05\n",
            "Epoch 78/150\n",
            "\u001b[1m983/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9723 - loss: 0.1898\n",
            "Epoch 78: val_loss improved from 0.22145 to 0.21944, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_78.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9723 - loss: 0.1898 - val_accuracy: 0.9637 - val_loss: 0.2194 - learning_rate: 5.0000e-05\n",
            "Epoch 79/150\n",
            "\u001b[1m994/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9717 - loss: 0.1888\n",
            "Epoch 79: val_loss improved from 0.21944 to 0.21803, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_79.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9717 - loss: 0.1888 - val_accuracy: 0.9631 - val_loss: 0.2180 - learning_rate: 5.0000e-05\n",
            "Epoch 80/150\n",
            "\u001b[1m995/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9713 - loss: 0.1931\n",
            "Epoch 80: val_loss did not improve from 0.21803\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9713 - loss: 0.1931 - val_accuracy: 0.9632 - val_loss: 0.2197 - learning_rate: 5.0000e-05\n",
            "Epoch 81/150\n",
            "\u001b[1m988/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9698 - loss: 0.1951\n",
            "Epoch 81: val_loss did not improve from 0.21803\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9698 - loss: 0.1951 - val_accuracy: 0.9636 - val_loss: 0.2198 - learning_rate: 5.0000e-05\n",
            "Epoch 82/150\n",
            "\u001b[1m988/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9718 - loss: 0.1889\n",
            "Epoch 82: val_loss improved from 0.21803 to 0.21563, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_82.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9718 - loss: 0.1889 - val_accuracy: 0.9652 - val_loss: 0.2156 - learning_rate: 5.0000e-05\n",
            "Epoch 83/150\n",
            "\u001b[1m993/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9718 - loss: 0.1855\n",
            "Epoch 83: val_loss did not improve from 0.21563\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9718 - loss: 0.1855 - val_accuracy: 0.9646 - val_loss: 0.2183 - learning_rate: 5.0000e-05\n",
            "Epoch 84/150\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9721 - loss: 0.1885\n",
            "Epoch 84: val_loss did not improve from 0.21563\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9721 - loss: 0.1885 - val_accuracy: 0.9652 - val_loss: 0.2167 - learning_rate: 5.0000e-05\n",
            "Epoch 85/150\n",
            "\u001b[1m992/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9710 - loss: 0.1855\n",
            "Epoch 85: val_loss improved from 0.21563 to 0.21256, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_85.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9710 - loss: 0.1855 - val_accuracy: 0.9646 - val_loss: 0.2126 - learning_rate: 5.0000e-05\n",
            "Epoch 86/150\n",
            "\u001b[1m994/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9727 - loss: 0.1839\n",
            "Epoch 86: val_loss did not improve from 0.21256\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.9727 - loss: 0.1839 - val_accuracy: 0.9632 - val_loss: 0.2158 - learning_rate: 5.0000e-05\n",
            "Epoch 87/150\n",
            "\u001b[1m985/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9714 - loss: 0.1875\n",
            "Epoch 87: val_loss did not improve from 0.21256\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9714 - loss: 0.1875 - val_accuracy: 0.9648 - val_loss: 0.2134 - learning_rate: 5.0000e-05\n",
            "Epoch 88/150\n",
            "\u001b[1m994/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9719 - loss: 0.1849\n",
            "Epoch 88: val_loss improved from 0.21256 to 0.21244, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_88.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9719 - loss: 0.1849 - val_accuracy: 0.9643 - val_loss: 0.2124 - learning_rate: 5.0000e-05\n",
            "Epoch 89/150\n",
            "\u001b[1m992/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9702 - loss: 0.1886\n",
            "Epoch 89: val_loss improved from 0.21244 to 0.21022, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_89.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9702 - loss: 0.1885 - val_accuracy: 0.9668 - val_loss: 0.2102 - learning_rate: 5.0000e-05\n",
            "Epoch 90/150\n",
            "\u001b[1m988/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9710 - loss: 0.1859\n",
            "Epoch 90: val_loss did not improve from 0.21022\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9710 - loss: 0.1859 - val_accuracy: 0.9647 - val_loss: 0.2125 - learning_rate: 5.0000e-05\n",
            "Epoch 91/150\n",
            "\u001b[1m985/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9733 - loss: 0.1763\n",
            "Epoch 91: val_loss improved from 0.21022 to 0.20949, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_91.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9733 - loss: 0.1763 - val_accuracy: 0.9642 - val_loss: 0.2095 - learning_rate: 5.0000e-05\n",
            "Epoch 92/150\n",
            "\u001b[1m984/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9731 - loss: 0.1785\n",
            "Epoch 92: val_loss improved from 0.20949 to 0.20924, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_92.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9731 - loss: 0.1785 - val_accuracy: 0.9652 - val_loss: 0.2092 - learning_rate: 5.0000e-05\n",
            "Epoch 93/150\n",
            "\u001b[1m994/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9727 - loss: 0.1758\n",
            "Epoch 93: val_loss improved from 0.20924 to 0.20912, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_93.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9727 - loss: 0.1758 - val_accuracy: 0.9646 - val_loss: 0.2091 - learning_rate: 5.0000e-05\n",
            "Epoch 94/150\n",
            "\u001b[1m987/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9719 - loss: 0.1777\n",
            "Epoch 94: val_loss did not improve from 0.20912\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9719 - loss: 0.1777 - val_accuracy: 0.9648 - val_loss: 0.2113 - learning_rate: 5.0000e-05\n",
            "Epoch 95/150\n",
            "\u001b[1m989/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9722 - loss: 0.1789\n",
            "Epoch 95: val_loss did not improve from 0.20912\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9722 - loss: 0.1789 - val_accuracy: 0.9641 - val_loss: 0.2102 - learning_rate: 5.0000e-05\n",
            "Epoch 96/150\n",
            "\u001b[1m988/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9721 - loss: 0.1775\n",
            "Epoch 96: val_loss did not improve from 0.20912\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9721 - loss: 0.1775 - val_accuracy: 0.9643 - val_loss: 0.2098 - learning_rate: 5.0000e-05\n",
            "Epoch 97/150\n",
            "\u001b[1m993/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9728 - loss: 0.1783\n",
            "Epoch 97: val_loss improved from 0.20912 to 0.20796, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_97.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9728 - loss: 0.1782 - val_accuracy: 0.9653 - val_loss: 0.2080 - learning_rate: 5.0000e-05\n",
            "Epoch 98/150\n",
            "\u001b[1m986/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9744 - loss: 0.1699\n",
            "Epoch 98: val_loss improved from 0.20796 to 0.20395, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_98.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9744 - loss: 0.1700 - val_accuracy: 0.9662 - val_loss: 0.2040 - learning_rate: 5.0000e-05\n",
            "Epoch 99/150\n",
            "\u001b[1m984/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9738 - loss: 0.1709\n",
            "Epoch 99: val_loss did not improve from 0.20395\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9738 - loss: 0.1710 - val_accuracy: 0.9662 - val_loss: 0.2042 - learning_rate: 5.0000e-05\n",
            "Epoch 100/150\n",
            "\u001b[1m984/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9724 - loss: 0.1759\n",
            "Epoch 100: val_loss did not improve from 0.20395\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9724 - loss: 0.1759 - val_accuracy: 0.9648 - val_loss: 0.2052 - learning_rate: 5.0000e-05\n",
            "Epoch 101/150\n",
            "\u001b[1m991/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9738 - loss: 0.1688\n",
            "Epoch 101: val_loss did not improve from 0.20395\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9738 - loss: 0.1689 - val_accuracy: 0.9651 - val_loss: 0.2092 - learning_rate: 5.0000e-05\n",
            "Epoch 102/150\n",
            "\u001b[1m991/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9725 - loss: 0.1720\n",
            "Epoch 102: val_loss improved from 0.20395 to 0.20322, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_102.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.9725 - loss: 0.1720 - val_accuracy: 0.9660 - val_loss: 0.2032 - learning_rate: 5.0000e-05\n",
            "Epoch 103/150\n",
            "\u001b[1m984/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9726 - loss: 0.1736\n",
            "Epoch 103: val_loss did not improve from 0.20322\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.9726 - loss: 0.1736 - val_accuracy: 0.9647 - val_loss: 0.2051 - learning_rate: 5.0000e-05\n",
            "Epoch 104/150\n",
            "\u001b[1m992/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9702 - loss: 0.1782\n",
            "Epoch 104: val_loss did not improve from 0.20322\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - accuracy: 0.9702 - loss: 0.1782 - val_accuracy: 0.9648 - val_loss: 0.2061 - learning_rate: 5.0000e-05\n",
            "Epoch 105/150\n",
            "\u001b[1m986/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9734 - loss: 0.1687\n",
            "Epoch 105: val_loss did not improve from 0.20322\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9734 - loss: 0.1688 - val_accuracy: 0.9646 - val_loss: 0.2045 - learning_rate: 5.0000e-05\n",
            "Epoch 106/150\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9727 - loss: 0.1685\n",
            "Epoch 106: val_loss improved from 0.20322 to 0.20137, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_106.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9727 - loss: 0.1685 - val_accuracy: 0.9655 - val_loss: 0.2014 - learning_rate: 5.0000e-05\n",
            "Epoch 107/150\n",
            "\u001b[1m987/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9721 - loss: 0.1729\n",
            "Epoch 107: val_loss did not improve from 0.20137\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9721 - loss: 0.1729 - val_accuracy: 0.9646 - val_loss: 0.2070 - learning_rate: 5.0000e-05\n",
            "Epoch 108/150\n",
            "\u001b[1m989/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9729 - loss: 0.1706\n",
            "Epoch 108: val_loss did not improve from 0.20137\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.9729 - loss: 0.1706 - val_accuracy: 0.9648 - val_loss: 0.2043 - learning_rate: 5.0000e-05\n",
            "Epoch 109/150\n",
            "\u001b[1m986/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9733 - loss: 0.1666\n",
            "Epoch 109: val_loss improved from 0.20137 to 0.20019, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_109.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9733 - loss: 0.1667 - val_accuracy: 0.9650 - val_loss: 0.2002 - learning_rate: 5.0000e-05\n",
            "Epoch 110/150\n",
            "\u001b[1m984/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9733 - loss: 0.1649\n",
            "Epoch 110: val_loss did not improve from 0.20019\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9733 - loss: 0.1650 - val_accuracy: 0.9667 - val_loss: 0.2016 - learning_rate: 5.0000e-05\n",
            "Epoch 111/150\n",
            "\u001b[1m993/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9742 - loss: 0.1623\n",
            "Epoch 111: val_loss did not improve from 0.20019\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - accuracy: 0.9742 - loss: 0.1623 - val_accuracy: 0.9657 - val_loss: 0.2006 - learning_rate: 5.0000e-05\n",
            "Epoch 112/150\n",
            "\u001b[1m986/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9730 - loss: 0.1653\n",
            "Epoch 112: val_loss did not improve from 0.20019\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9730 - loss: 0.1653 - val_accuracy: 0.9668 - val_loss: 0.2024 - learning_rate: 5.0000e-05\n",
            "Epoch 113/150\n",
            "\u001b[1m990/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9735 - loss: 0.1655\n",
            "Epoch 113: val_loss improved from 0.20019 to 0.19948, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_113.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9735 - loss: 0.1655 - val_accuracy: 0.9642 - val_loss: 0.1995 - learning_rate: 5.0000e-05\n",
            "Epoch 114/150\n",
            "\u001b[1m993/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9736 - loss: 0.1639\n",
            "Epoch 114: val_loss did not improve from 0.19948\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9736 - loss: 0.1640 - val_accuracy: 0.9656 - val_loss: 0.2021 - learning_rate: 5.0000e-05\n",
            "Epoch 115/150\n",
            "\u001b[1m989/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9729 - loss: 0.1669\n",
            "Epoch 115: val_loss did not improve from 0.19948\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9729 - loss: 0.1669 - val_accuracy: 0.9640 - val_loss: 0.2044 - learning_rate: 5.0000e-05\n",
            "Epoch 116/150\n",
            "\u001b[1m991/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9730 - loss: 0.1678\n",
            "Epoch 116: val_loss improved from 0.19948 to 0.19869, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_116.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.9730 - loss: 0.1677 - val_accuracy: 0.9660 - val_loss: 0.1987 - learning_rate: 5.0000e-05\n",
            "Epoch 117/150\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9719 - loss: 0.1705\n",
            "Epoch 117: val_loss improved from 0.19869 to 0.19708, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_117.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - accuracy: 0.9719 - loss: 0.1705 - val_accuracy: 0.9681 - val_loss: 0.1971 - learning_rate: 5.0000e-05\n",
            "Epoch 118/150\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9741 - loss: 0.1624\n",
            "Epoch 118: val_loss did not improve from 0.19708\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9741 - loss: 0.1624 - val_accuracy: 0.9680 - val_loss: 0.1988 - learning_rate: 5.0000e-05\n",
            "Epoch 119/150\n",
            "\u001b[1m995/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9742 - loss: 0.1606\n",
            "Epoch 119: val_loss improved from 0.19708 to 0.19627, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_119.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9742 - loss: 0.1606 - val_accuracy: 0.9670 - val_loss: 0.1963 - learning_rate: 5.0000e-05\n",
            "Epoch 120/150\n",
            "\u001b[1m995/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9746 - loss: 0.1616\n",
            "Epoch 120: val_loss did not improve from 0.19627\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9746 - loss: 0.1616 - val_accuracy: 0.9653 - val_loss: 0.1973 - learning_rate: 5.0000e-05\n",
            "Epoch 121/150\n",
            "\u001b[1m989/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9734 - loss: 0.1622\n",
            "Epoch 121: val_loss improved from 0.19627 to 0.19478, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_121.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9734 - loss: 0.1622 - val_accuracy: 0.9681 - val_loss: 0.1948 - learning_rate: 5.0000e-05\n",
            "Epoch 122/150\n",
            "\u001b[1m985/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9743 - loss: 0.1641\n",
            "Epoch 122: val_loss did not improve from 0.19478\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.9743 - loss: 0.1641 - val_accuracy: 0.9656 - val_loss: 0.1952 - learning_rate: 5.0000e-05\n",
            "Epoch 123/150\n",
            "\u001b[1m987/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9748 - loss: 0.1604\n",
            "Epoch 123: val_loss did not improve from 0.19478\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9748 - loss: 0.1604 - val_accuracy: 0.9660 - val_loss: 0.1962 - learning_rate: 5.0000e-05\n",
            "Epoch 124/150\n",
            "\u001b[1m993/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9740 - loss: 0.1594\n",
            "Epoch 124: val_loss improved from 0.19478 to 0.19388, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_124.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.9740 - loss: 0.1594 - val_accuracy: 0.9652 - val_loss: 0.1939 - learning_rate: 5.0000e-05\n",
            "Epoch 125/150\n",
            "\u001b[1m995/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9743 - loss: 0.1569\n",
            "Epoch 125: val_loss did not improve from 0.19388\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9743 - loss: 0.1569 - val_accuracy: 0.9665 - val_loss: 0.1958 - learning_rate: 5.0000e-05\n",
            "Epoch 126/150\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9742 - loss: 0.1589\n",
            "Epoch 126: val_loss improved from 0.19388 to 0.19075, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_126.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9742 - loss: 0.1589 - val_accuracy: 0.9663 - val_loss: 0.1907 - learning_rate: 5.0000e-05\n",
            "Epoch 127/150\n",
            "\u001b[1m985/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9740 - loss: 0.1576\n",
            "Epoch 127: val_loss improved from 0.19075 to 0.19050, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_127.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9740 - loss: 0.1576 - val_accuracy: 0.9647 - val_loss: 0.1905 - learning_rate: 5.0000e-05\n",
            "Epoch 128/150\n",
            "\u001b[1m989/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9742 - loss: 0.1580\n",
            "Epoch 128: val_loss did not improve from 0.19050\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9742 - loss: 0.1580 - val_accuracy: 0.9658 - val_loss: 0.1907 - learning_rate: 5.0000e-05\n",
            "Epoch 129/150\n",
            "\u001b[1m995/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9736 - loss: 0.1579\n",
            "Epoch 129: val_loss did not improve from 0.19050\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9736 - loss: 0.1579 - val_accuracy: 0.9655 - val_loss: 0.1968 - learning_rate: 5.0000e-05\n",
            "Epoch 130/150\n",
            "\u001b[1m990/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9739 - loss: 0.1594\n",
            "Epoch 130: val_loss did not improve from 0.19050\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9740 - loss: 0.1594 - val_accuracy: 0.9666 - val_loss: 0.1945 - learning_rate: 5.0000e-05\n",
            "Epoch 131/150\n",
            "\u001b[1m986/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9749 - loss: 0.1523\n",
            "Epoch 131: val_loss did not improve from 0.19050\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9749 - loss: 0.1524 - val_accuracy: 0.9656 - val_loss: 0.1921 - learning_rate: 5.0000e-05\n",
            "Epoch 132/150\n",
            "\u001b[1m992/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9757 - loss: 0.1532\n",
            "Epoch 132: val_loss did not improve from 0.19050\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9757 - loss: 0.1532 - val_accuracy: 0.9668 - val_loss: 0.1937 - learning_rate: 5.0000e-05\n",
            "Epoch 133/150\n",
            "\u001b[1m995/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9757 - loss: 0.1497\n",
            "Epoch 133: val_loss improved from 0.19050 to 0.18485, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_133.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9757 - loss: 0.1497 - val_accuracy: 0.9679 - val_loss: 0.1849 - learning_rate: 5.0000e-06\n",
            "Epoch 134/150\n",
            "\u001b[1m987/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9762 - loss: 0.1483\n",
            "Epoch 134: val_loss did not improve from 0.18485\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.9762 - loss: 0.1483 - val_accuracy: 0.9675 - val_loss: 0.1849 - learning_rate: 5.0000e-06\n",
            "Epoch 135/150\n",
            "\u001b[1m994/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9766 - loss: 0.1477\n",
            "Epoch 135: val_loss improved from 0.18485 to 0.18444, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_135.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9766 - loss: 0.1477 - val_accuracy: 0.9679 - val_loss: 0.1844 - learning_rate: 5.0000e-06\n",
            "Epoch 136/150\n",
            "\u001b[1m985/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9762 - loss: 0.1474\n",
            "Epoch 136: val_loss improved from 0.18444 to 0.18419, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_136.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9762 - loss: 0.1475 - val_accuracy: 0.9667 - val_loss: 0.1842 - learning_rate: 5.0000e-06\n",
            "Epoch 137/150\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9763 - loss: 0.1460\n",
            "Epoch 137: val_loss did not improve from 0.18419\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9763 - loss: 0.1460 - val_accuracy: 0.9685 - val_loss: 0.1851 - learning_rate: 5.0000e-06\n",
            "Epoch 138/150\n",
            "\u001b[1m985/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9755 - loss: 0.1530\n",
            "Epoch 138: val_loss improved from 0.18419 to 0.18392, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_138.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9755 - loss: 0.1530 - val_accuracy: 0.9677 - val_loss: 0.1839 - learning_rate: 5.0000e-06\n",
            "Epoch 139/150\n",
            "\u001b[1m995/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9757 - loss: 0.1506\n",
            "Epoch 139: val_loss did not improve from 0.18392\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9757 - loss: 0.1506 - val_accuracy: 0.9668 - val_loss: 0.1841 - learning_rate: 5.0000e-06\n",
            "Epoch 140/150\n",
            "\u001b[1m995/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9767 - loss: 0.1443\n",
            "Epoch 140: val_loss did not improve from 0.18392\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.9767 - loss: 0.1443 - val_accuracy: 0.9677 - val_loss: 0.1845 - learning_rate: 5.0000e-06\n",
            "Epoch 141/150\n",
            "\u001b[1m991/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9764 - loss: 0.1465\n",
            "Epoch 141: val_loss did not improve from 0.18392\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - accuracy: 0.9764 - loss: 0.1465 - val_accuracy: 0.9681 - val_loss: 0.1847 - learning_rate: 5.0000e-06\n",
            "Epoch 142/150\n",
            "\u001b[1m994/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9757 - loss: 0.1485\n",
            "Epoch 142: val_loss did not improve from 0.18392\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9757 - loss: 0.1485 - val_accuracy: 0.9665 - val_loss: 0.1847 - learning_rate: 5.0000e-06\n",
            "Epoch 143/150\n",
            "\u001b[1m986/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9767 - loss: 0.1470\n",
            "Epoch 143: val_loss improved from 0.18392 to 0.18361, saving model to /content/drive/MyDrive/Semester 5/DL/Project/checkpoint/transformer_model_epoch_143.keras\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.9767 - loss: 0.1470 - val_accuracy: 0.9680 - val_loss: 0.1836 - learning_rate: 5.0000e-06\n",
            "Epoch 144/150\n",
            "\u001b[1m993/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9753 - loss: 0.1512\n",
            "Epoch 144: val_loss did not improve from 0.18361\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9753 - loss: 0.1512 - val_accuracy: 0.9687 - val_loss: 0.1839 - learning_rate: 5.0000e-06\n",
            "Epoch 145/150\n",
            "\u001b[1m985/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9764 - loss: 0.1445\n",
            "Epoch 145: val_loss did not improve from 0.18361\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9764 - loss: 0.1445 - val_accuracy: 0.9680 - val_loss: 0.1839 - learning_rate: 5.0000e-06\n",
            "Epoch 146/150\n",
            "\u001b[1m992/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9756 - loss: 0.1494\n",
            "Epoch 146: val_loss did not improve from 0.18361\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9756 - loss: 0.1494 - val_accuracy: 0.9680 - val_loss: 0.1839 - learning_rate: 5.0000e-06\n",
            "Epoch 147/150\n",
            "\u001b[1m986/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9751 - loss: 0.1516\n",
            "Epoch 147: val_loss did not improve from 0.18361\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - accuracy: 0.9751 - loss: 0.1515 - val_accuracy: 0.9692 - val_loss: 0.1843 - learning_rate: 5.0000e-06\n",
            "Epoch 148/150\n",
            "\u001b[1m992/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9769 - loss: 0.1455\n",
            "Epoch 148: val_loss did not improve from 0.18361\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9769 - loss: 0.1455 - val_accuracy: 0.9676 - val_loss: 0.1838 - learning_rate: 5.0000e-06\n",
            "Epoch 149/150\n",
            "\u001b[1m987/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9768 - loss: 0.1409\n",
            "Epoch 149: val_loss did not improve from 0.18361\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9768 - loss: 0.1410 - val_accuracy: 0.9672 - val_loss: 0.1836 - learning_rate: 5.0000e-07\n",
            "Epoch 150/150\n",
            "\u001b[1m984/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9762 - loss: 0.1473\n",
            "Epoch 150: val_loss did not improve from 0.18361\n",
            "\u001b[1m996/996\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9762 - loss: 0.1473 - val_accuracy: 0.9675 - val_loss: 0.1838 - learning_rate: 5.0000e-07\n",
            "\u001b[1m249/249\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9704 - loss: 0.1715\n",
            "Test Loss: 0.18360650539398193, Test Accuracy: 0.9679768681526184\n"
          ]
        }
      ],
      "source": [
        "# Define the Transformer Encoder Block\n",
        "def transformer_encoder(inputs, num_heads, ff_dim, dropout_rate, l2_lambda):\n",
        "    # Multi-Head Attention\n",
        "    attention_output = MultiHeadAttention(num_heads=num_heads, key_dim=inputs.shape[-1])(inputs, inputs)\n",
        "    attention_output = Dropout(dropout_rate)(attention_output)\n",
        "    attention_output = Add()([inputs, attention_output])  # Residual connection\n",
        "    attention_output = LayerNormalization()(attention_output)\n",
        "\n",
        "    # Feed-Forward Network\n",
        "    ffn_output = Dense(ff_dim, activation='relu', kernel_regularizer=l2(l2_lambda))(attention_output)\n",
        "    ffn_output = Dense(inputs.shape[-1], kernel_regularizer=l2(l2_lambda))(ffn_output)\n",
        "    ffn_output = Dropout(dropout_rate)(ffn_output)\n",
        "    ffn_output = Add()([attention_output, ffn_output])  # Residual connection\n",
        "    ffn_output = LayerNormalization()(ffn_output)\n",
        "\n",
        "    return ffn_output\n",
        "\n",
        "# Create a directory to save the checkpoints\n",
        "checkpoint_dir = '/content/drive/MyDrive/Semester 5/DL/Project/checkpoint'  # Updated path\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "# Define the learning rate and optimizer\n",
        "learning_rate = 0.0005  # Adjusted learning rate\n",
        "l2_lambda = 0.001\n",
        "optimizer = AdamW(learning_rate=learning_rate, weight_decay=0.01)\n",
        "\n",
        "# Define the input shape\n",
        "input_layer = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
        "\n",
        "# Positional Encoding\n",
        "positional_encoding = Dense(X_train.shape[2], activation='relu')(input_layer)\n",
        "\n",
        "# 1. Add Conv1D layer to capture local patterns\n",
        "conv1d_layer = Conv1D(filters=64, kernel_size=3, activation='relu')(positional_encoding)\n",
        "conv1d_layer = BatchNormalization()(conv1d_layer)\n",
        "\n",
        "# 2. Add Transformer Encoder blocks\n",
        "transformer_output = transformer_encoder(conv1d_layer, num_heads=4, ff_dim=128, dropout_rate=0.3, l2_lambda=l2_lambda)\n",
        "transformer_output = transformer_encoder(transformer_output, num_heads=4, ff_dim=128, dropout_rate=0.3, l2_lambda=l2_lambda)\n",
        "\n",
        "# 3. Global Average Pooling to reduce sequence dimensions\n",
        "global_pool = GlobalAveragePooling1D()(transformer_output)\n",
        "\n",
        "# 4. Output layer\n",
        "output_layer = Dense(num_classes, activation='softmax', kernel_regularizer=l2(l2_lambda))(global_pool)\n",
        "\n",
        "# Define the model\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()\n",
        "\n",
        "# Implement callbacks\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.1,\n",
        "    patience=5,\n",
        "    min_lr=1e-7\n",
        ")\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=12,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Add ModelCheckpoint callback\n",
        "checkpoint_path = os.path.join(checkpoint_dir, 'transformer_model_epoch_{epoch:02d}.keras')\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=checkpoint_path,\n",
        "    save_weights_only=False,\n",
        "    save_best_only=True,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=150,\n",
        "    batch_size=32,\n",
        "    validation_data=(X_test, y_test),\n",
        "    callbacks=[reduce_lr, early_stopping, checkpoint_callback],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3q21Z_yg-lu",
        "outputId": "cf518834-a76e-48cb-a549-de67ecd49e1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ],
      "source": [
        "model.save('model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "B2y74j82g-lu",
        "outputId": "0fdd25aa-7a9a-4482-e50c-3a30c9627688"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m249/249\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.1836\n",
            "Test Accuracy: 0.9680\n",
            "Top-1 Error: 0.0320\n",
            "Top-5 Error: 0.0183\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    accident       0.99      0.96      0.97        98\n",
            "       apple       1.00      1.00      1.00       115\n",
            "       argue       0.98      0.98      0.98        85\n",
            "         bad       1.00      0.94      0.97        63\n",
            "     balance       0.95      0.98      0.97       100\n",
            "         bar       1.00      0.96      0.98        95\n",
            "  basketball       1.00      0.99      0.99        92\n",
            "     because       1.00      0.98      0.99        56\n",
            "         bed       0.96      1.00      0.98        76\n",
            "      before       0.97      0.92      0.94       102\n",
            "        bird       1.00      0.98      0.99        85\n",
            "       black       0.98      0.94      0.96        68\n",
            "     blanket       1.00      0.95      0.98        63\n",
            "     bowling       1.00      0.99      0.99        76\n",
            "     brother       0.99      0.98      0.98        86\n",
            "        call       1.00      1.00      1.00        57\n",
            "       candy       0.88      1.00      0.93        85\n",
            "    champion       1.00      0.96      0.98        45\n",
            "      change       1.00      1.00      1.00        94\n",
            "       cheat       1.00      0.96      0.98        54\n",
            "       check       1.00      0.97      0.98        99\n",
            "        cold       0.99      0.97      0.98        73\n",
            "    computer       1.00      0.98      0.99       109\n",
            "    convince       1.00      0.95      0.97        61\n",
            "        cool       0.99      0.98      0.98        94\n",
            "        corn       0.99      0.98      0.98        88\n",
            "      cousin       0.83      0.92      0.87       140\n",
            "         cry       1.00      1.00      1.00        55\n",
            "        dark       1.00      0.96      0.98        80\n",
            "    daughter       0.98      0.98      0.98        63\n",
            "        deaf       1.00      1.00      1.00        53\n",
            "       delay       1.00      0.98      0.99        51\n",
            "   delicious       1.00      0.97      0.98        59\n",
            "      doctor       1.00      0.95      0.97        94\n",
            "         dog       1.00      0.94      0.97        88\n",
            "       drink       1.00      0.93      0.97       106\n",
            " environment       0.99      0.98      0.98        99\n",
            "     example       1.00      0.98      0.99        62\n",
            "      family       1.00      1.00      1.00        97\n",
            "         far       1.00      1.00      1.00        71\n",
            "         fat       0.99      0.95      0.97        74\n",
            "        fish       0.97      1.00      0.99        69\n",
            "        full       1.00      0.97      0.99        72\n",
            "        give       0.98      0.98      0.98        50\n",
            "          go       0.90      0.93      0.91        84\n",
            "        good       1.00      1.00      1.00        49\n",
            "  government       0.83      1.00      0.90        76\n",
            "    graduate       0.98      0.95      0.96        57\n",
            "        help       1.00      1.00      1.00        86\n",
            "         hot       1.00      0.96      0.98        54\n",
            "    interest       0.99      0.97      0.98       111\n",
            "    language       1.00      1.00      1.00        63\n",
            "        last       0.93      0.96      0.94        80\n",
            "       later       0.57      0.99      0.72        90\n",
            "       laugh       1.00      0.94      0.97        65\n",
            "       leave       0.98      1.00      0.99        65\n",
            "      letter       0.98      0.98      0.98        64\n",
            "        like       0.96      0.97      0.97        76\n",
            "         man       1.00      0.86      0.93       102\n",
            "        many       1.00      0.97      0.98        59\n",
            "      mother       0.94      1.00      0.97        85\n",
            "        move       1.00      0.95      0.98        63\n",
            "          no       0.99      0.99      0.99        80\n",
            "      orange       0.98      0.97      0.97        87\n",
            "       order       0.99      0.97      0.98        78\n",
            " perspective       1.00      0.98      0.99        90\n",
            "       pizza       1.00      0.98      0.99        84\n",
            "        play       0.99      0.98      0.98        98\n",
            "        room       0.98      0.98      0.98        66\n",
            "    sandwich       1.00      0.94      0.97        63\n",
            "       score       0.98      0.94      0.96        88\n",
            "   secretary       1.00      0.95      0.97        75\n",
            "       shirt       0.99      0.98      0.99       122\n",
            "       short       0.97      0.94      0.95        95\n",
            "       silly       0.99      0.95      0.97        86\n",
            "        snow       1.00      1.00      1.00        96\n",
            "         son       1.00      0.97      0.98        59\n",
            "        soon       0.93      0.99      0.96        77\n",
            "       study       1.00      0.96      0.98       106\n",
            "       sweet       0.98      0.95      0.97        62\n",
            "        take       1.00      0.93      0.96        73\n",
            "        tall       0.97      0.94      0.95        93\n",
            "        tell       0.98      0.91      0.94        53\n",
            "thanksgiving       0.99      0.99      0.99       112\n",
            "      theory       0.96      0.90      0.93        90\n",
            "        thin       0.88      0.96      0.92       106\n",
            "    thursday       0.99      0.98      0.98        83\n",
            "       trade       0.99      0.97      0.98        88\n",
            "        wait       0.98      0.98      0.98        85\n",
            "        walk       0.90      1.00      0.95        79\n",
            "        what       0.97      0.94      0.95        78\n",
            "       white       0.88      0.99      0.93        68\n",
            "         who       0.98      0.93      0.95       111\n",
            "         why       0.91      1.00      0.95        69\n",
            "       woman       0.93      1.00      0.97        69\n",
            "        work       1.00      0.95      0.97        73\n",
            "       write       0.99      0.96      0.97        79\n",
            "        year       1.00      1.00      1.00        70\n",
            "         yes       1.00      1.00      1.00        71\n",
            "   yesterday       1.00      0.94      0.97        70\n",
            "\n",
            "    accuracy                           0.97      7963\n",
            "   macro avg       0.98      0.97      0.97      7963\n",
            "weighted avg       0.97      0.97      0.97      7963\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import top_k_accuracy_score\n",
        "\n",
        "# Get predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate Top-1 Error\n",
        "top1_accuracy = np.mean(np.argmax(y_pred, axis=1) == np.argmax(y_test, axis=1))\n",
        "top1_error = 1 - top1_accuracy\n",
        "\n",
        "# Calculate Top-5 Error\n",
        "top5_accuracy = top_k_accuracy_score(np.argmax(y_test, axis=1), y_pred, k=5)\n",
        "top5_error = 1 - top5_accuracy\n",
        "\n",
        "# If you want to plot the training history including these metrics:\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print final evaluation metrics\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Top-1 Error: {top1_error:.4f}\")\n",
        "print(f\"Top-5 Error: {top5_error:.4f}\")\n",
        "\n",
        "# If you want to see per-class accuracy:\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "class_report = classification_report(y_true, y_pred_classes, target_names=label_encoder.classes_)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(class_report)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}